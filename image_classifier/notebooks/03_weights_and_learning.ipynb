{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重みパラメータとバイアス、そして学習\n",
    "\n",
    "このノートブックでは、CNNの**重みパラメータ**と**バイアス**、そして**学習**の概念を理解します。\n",
    "\n",
    "## 目次\n",
    "1. 畳み込みフィルタのスライド（図3.9）\n",
    "2. 重みパラメータとバイアスの表記（図3.10）\n",
    "3. 予測と学習の違い（図3.11）\n",
    "4. ReLU活性化関数\n",
    "5. 具体的な計算式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 畳み込みフィルタのスライド（図3.9）\n",
    "\n",
    "24×24の入力画像に対して、5×5のフィルタをstride=1で適用すると：\n",
    "\n",
    "$$\\text{出力サイズ} = \\frac{24 - 5}{1} + 1 = 20$$\n",
    "\n",
    "つまり、20×20の特徴マップが生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図3.9: フィルタのスライドを視覚化\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "input_size = 24\n",
    "filter_size = 5\n",
    "\n",
    "# 4つの位置でフィルタの移動を表示\n",
    "positions = [(0, 0), (0, 5), (5, 0), (19, 19)]  # 左上、少し右、少し下、右下\n",
    "titles = ['位置(0,0)\\n左上', '位置(0,5)\\n5ピクセル右', '位置(5,0)\\n5ピクセル下', '位置(19,19)\\n右下（最後）']\n",
    "\n",
    "for idx, ((row, col), title) in enumerate(zip(positions, titles)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # 入力画像（24×24）\n",
    "    img = np.zeros((input_size, input_size))\n",
    "    ax.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "    \n",
    "    # フィルタの位置を示す赤枠\n",
    "    from matplotlib.patches import Rectangle\n",
    "    rect = Rectangle((col-0.5, row-0.5), filter_size, filter_size, \n",
    "                      fill=False, edgecolor='red', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xlim(-0.5, input_size-0.5)\n",
    "    ax.set_ylim(input_size-0.5, -0.5)\n",
    "    ax.set_xticks([0, 23])\n",
    "    ax.set_yticks([0, 23])\n",
    "\n",
    "plt.suptitle('図3.9: 5×5フィルタが24×24画像上をスライド → 20×20の出力', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"入力サイズ: {input_size}×{input_size}\")\n",
    "print(f\"フィルタサイズ: {filter_size}×{filter_size}\")\n",
    "print(f\"出力サイズ: ({input_size}-{filter_size})/1+1 = {input_size - filter_size + 1}×{input_size - filter_size + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 重みパラメータとバイアスの表記（図3.10）\n",
    "\n",
    "### 記号の意味\n",
    "\n",
    "| 記号 | 意味 | 例 |\n",
    "|-----|------|----|\n",
    "| $x_{i,j}$ | 入力データのi行j列目 | $x_{2,3}$は2行3列目のピクセル値 |\n",
    "| $w^k_{i,j}$ | フィルタk枚目のi行j列目の**重み** | $w^1_{2,3}$はフィルタ1枚目の2行3列目 |\n",
    "| $c^k_{i,j}$ | 特徴マップk枚目のi行j列目の**出力** | $c^1_{2,3}$は特徴マップ1枚目の2行3列目 |\n",
    "| $b_k$ | フィルタk枚目の**バイアス** | $b_1$はフィルタ1枚目のバイアス |\n",
    "\n",
    "### パラメータ数の計算\n",
    "\n",
    "5×5のフィルタを2枚使う場合：\n",
    "- 重みパラメータ: $5 \\times 5 \\times 2 = 50$個\n",
    "- バイアス: $2$個\n",
    "- **合計: 52個**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ数の計算\n",
    "filter_h, filter_w = 5, 5\n",
    "num_filters = 2\n",
    "\n",
    "weight_params = filter_h * filter_w * num_filters\n",
    "bias_params = num_filters\n",
    "total_params = weight_params + bias_params\n",
    "\n",
    "print(\"=== 畳み込み層（1回目）のパラメータ数 ===\")\n",
    "print(f\"フィルタサイズ: {filter_h}×{filter_w}\")\n",
    "print(f\"フィルタ枚数: {num_filters}\")\n",
    "print(f\"\")\n",
    "print(f\"重みパラメータ: {filter_h}×{filter_w}×{num_filters} = {weight_params}個\")\n",
    "print(f\"バイアス: {num_filters}個\")\n",
    "print(f\"合計: {total_params}個\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 予測と学習の違い（図3.11）\n",
    "\n",
    "一次関数 $y = ax + b$ で考えてみましょう。\n",
    "\n",
    "| | 予測 | 学習 |\n",
    "|--|------|------|\n",
    "| 目的 | 入力xから出力yを計算 | パラメータa,bを最適化 |\n",
    "| パラメータa,b | 固定 | 更新される |\n",
    "| 使うデータ | 入力x | 入力x、予測値y、正解値 |\n",
    "\n",
    "### 機械学習の本質\n",
    "**学習** = パラメータを最適化して、予測値の精度を向上させること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図3.11: 予測と学習の違いを視覚化\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 左: 予測\n",
    "ax = axes[0]\n",
    "x = np.linspace(0, 5, 100)\n",
    "a, b = 1.5, 1  # 固定されたパラメータ\n",
    "y = a * x + b\n",
    "ax.plot(x, y, 'b-', linewidth=2, label=f'y = {a}x + {b}')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.scatter([2], [a*2+b], color='red', s=100, zorder=5)\n",
    "ax.annotate(f'入力x=2 → 出力y={a*2+b}', xy=(2, a*2+b), xytext=(2.5, a*2+b+1),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10)\n",
    "ax.set_xlabel('x（入力）')\n",
    "ax.set_ylabel('y（出力）')\n",
    "ax.set_title('予測\\nパラメータa,bは固定\\n入力xから出力yを計算', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.5, 5)\n",
    "ax.set_ylim(-0.5, 10)\n",
    "\n",
    "# 右: 学習\n",
    "ax = axes[1]\n",
    "# 正解データ（散布点）\n",
    "np.random.seed(42)\n",
    "x_data = np.array([1, 2, 3, 4])\n",
    "y_true = 2 * x_data + 0.5 + np.random.randn(4) * 0.3\n",
    "ax.scatter(x_data, y_true, color='green', s=100, label='正解データ', zorder=5)\n",
    "\n",
    "# 学習前の予測線\n",
    "ax.plot(x, 1.0 * x + 2, 'r--', linewidth=1, alpha=0.5, label='学習前: y=1.0x+2')\n",
    "# 学習後の予測線\n",
    "ax.plot(x, 2.0 * x + 0.5, 'b-', linewidth=2, label='学習後: y=2.0x+0.5')\n",
    "\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x（入力）')\n",
    "ax.set_ylabel('y（出力）')\n",
    "ax.set_title('学習\\n正解データを使って\\nパラメータa,bを更新', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.5, 5)\n",
    "ax.set_ylim(-0.5, 10)\n",
    "\n",
    "plt.suptitle('図3.11: AIによる「予測」と「学習」の違い', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ReLU活性化関数\n",
    "\n",
    "畳み込み演算の結果に適用される**活性化関数**です。\n",
    "\n",
    "$$f(x) = \\begin{cases} 0 & (x < 0) \\\\ x & (x \\geq 0) \\end{cases}$$\n",
    "\n",
    "### 意味\n",
    "- **負の値** → 0にカット\n",
    "- **正の値** → そのまま通す\n",
    "\n",
    "### なぜ必要？\n",
    "- 非線形性を導入（線形変換だけでは複雑なパターンを学習できない）\n",
    "- 計算が単純で高速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU関数の実装と可視化\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU活性化関数\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 左: ReLU関数のグラフ\n",
    "ax = axes[0]\n",
    "ax.plot(x, y, 'b-', linewidth=2)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.fill_between(x[x<0], y[x<0], alpha=0.3, color='red', label='x<0 → 0')\n",
    "ax.fill_between(x[x>=0], 0, y[x>=0], alpha=0.3, color='blue', label='x≥0 → x')\n",
    "ax.set_xlabel('x（入力）')\n",
    "ax.set_ylabel('f(x)（出力）')\n",
    "ax.set_title('ReLU関数: f(x) = max(0, x)', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-1, 5)\n",
    "\n",
    "# 右: 具体例\n",
    "ax = axes[1]\n",
    "examples = [-3, -1, 0, 2, 5]\n",
    "results = [relu(e) for e in examples]\n",
    "colors = ['red' if e < 0 else 'blue' for e in examples]\n",
    "\n",
    "bars = ax.bar(range(len(examples)), results, color=colors, alpha=0.7)\n",
    "ax.set_xticks(range(len(examples)))\n",
    "ax.set_xticklabels([f'x={e}\\n→{r}' for e, r in zip(examples, results)])\n",
    "ax.set_ylabel('ReLU(x)')\n",
    "ax.set_title('ReLUの計算例', fontsize=11)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ReLU関数の計算例:\")\n",
    "for e in examples:\n",
    "    print(f\"  ReLU({e:2d}) = {relu(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 具体的な計算式\n",
    "\n",
    "特徴マップの出力 $c^1_{1,1}$ は以下のように計算されます：\n",
    "\n",
    "$$c^1_{1,1} = f\\left(\\sum_{i=1}^{5}\\sum_{j=1}^{5} w^1_{i,j} \\cdot x_{i,j} + b_1\\right)$$\n",
    "\n",
    "展開すると：\n",
    "$$c^1_{1,1} = f(w^1_{1,1}x_{1,1} + w^1_{1,2}x_{1,2} + \\cdots + w^1_{5,5}x_{5,5} + b_1)$$\n",
    "\n",
    "つまり：\n",
    "1. 5×5=25個の積和演算\n",
    "2. バイアス$b_1$を加算\n",
    "3. ReLU関数$f$を適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体的な計算例\n",
    "\n",
    "# ランダムな入力データ（5×5領域）\n",
    "np.random.seed(42)\n",
    "x = np.random.randint(0, 10, (5, 5))\n",
    "\n",
    "# ランダムな重みパラメータ\n",
    "w = np.random.randn(5, 5).round(2)\n",
    "\n",
    "# バイアス\n",
    "b = 0.5\n",
    "\n",
    "print(\"入力データ x (5×5):\")\n",
    "print(x)\n",
    "print(\"\\n重みパラメータ w (5×5):\")\n",
    "print(w)\n",
    "print(f\"\\nバイアス b = {b}\")\n",
    "\n",
    "# 積和演算\n",
    "dot_product = np.sum(x * w)\n",
    "print(f\"\\n積和演算: Σ(x * w) = {dot_product:.2f}\")\n",
    "\n",
    "# バイアス加算\n",
    "with_bias = dot_product + b\n",
    "print(f\"バイアス加算: {dot_product:.2f} + {b} = {with_bias:.2f}\")\n",
    "\n",
    "# ReLU適用\n",
    "output = relu(with_bias)\n",
    "print(f\"ReLU適用: f({with_bias:.2f}) = {output:.2f}\")\n",
    "print(f\"\\n最終出力 c = {output:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchでの実装例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 畳み込み層 + ReLU\n",
    "conv_relu = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5, stride=1, bias=True),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# パラメータ数を確認\n",
    "total_params = sum(p.numel() for p in conv_relu.parameters())\n",
    "print(f\"総パラメータ数: {total_params}\")\n",
    "print(f\"  - 重み: 5×5×1×2 = {5*5*1*2}\")\n",
    "print(f\"  - バイアス: 2\")\n",
    "\n",
    "# 24×24の入力に適用\n",
    "dummy_input = torch.randn(1, 1, 24, 24)\n",
    "output = conv_relu(dummy_input)\n",
    "print(f\"\\n入力サイズ: {dummy_input.shape}\")\n",
    "print(f\"出力サイズ: {output.shape}  # (batch, channels, height, width)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "### 重みパラメータとバイアス\n",
    "- **重みパラメータ $w$**: フィルタの各要素の値（学習で更新される）\n",
    "- **バイアス $b$**: 積和演算結果に加算する定数（学習で更新される）\n",
    "- パラメータ数 = フィルタサイズ × フィルタ枚数 + バイアス数\n",
    "\n",
    "### 予測と学習\n",
    "- **予測**: パラメータ固定で入力から出力を計算\n",
    "- **学習**: 誤差を最小化するようにパラメータを更新\n",
    "\n",
    "### ReLU活性化関数\n",
    "- $f(x) = \\max(0, x)$\n",
    "- 負の値を0にカット、正の値はそのまま\n",
    "- 非線形性を導入\n",
    "\n",
    "## 次のステップ\n",
    "\n",
    "次のノートブックでは、**誤差逆伝播法**による学習のメカニズムを学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
