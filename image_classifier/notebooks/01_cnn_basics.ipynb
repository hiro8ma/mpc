{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN（畳み込みニューラルネットワーク）の基礎\n",
    "\n",
    "このノートブックでは、画像分類の数理的基礎を学びます。\n",
    "\n",
    "## 目次\n",
    "1. 画像データの構造\n",
    "2. なぜCNNが必要か？（特徴量の保持）\n",
    "3. CNNの全体像\n",
    "4. 各層の役割\n",
    "5. 手書き数字分類（MNIST）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 画像データの構造\n",
    "\n",
    "画像データは**ピクセル**（画素）と呼ばれる微小な四角形の集合体です。\n",
    "\n",
    "| 画像タイプ | データ構造 | ピクセル値 |\n",
    "|-----------|-----------|----------|\n",
    "| グレースケール | 2次元 (H, W) | 0（黒）〜 255（白）|\n",
    "| カラー（RGB） | 3次元 (H, W, 3) | 各チャンネル 0〜255 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 図3.2: グレースケール画像のピクセル値の例（教科書より）\n",
    "# 0〜255の値で濃淡を表現\n",
    "pixel_example = np.array([\n",
    "    [0,   85,  170],\n",
    "    [255, 50,  100],\n",
    "    [150, 200, 225]\n",
    "], dtype=np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 左: ピクセル値を数値で表示\n",
    "ax1 = axes[0]\n",
    "ax1.imshow(pixel_example, cmap='gray', vmin=0, vmax=255)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax1.text(j, i, str(pixel_example[i, j]), \n",
    "                ha='center', va='center', fontsize=16,\n",
    "                color='white' if pixel_example[i, j] < 128 else 'black')\n",
    "ax1.set_title('グレースケール画像のピクセル値\\n(0=黒, 255=白)', fontsize=12)\n",
    "ax1.axis('off')\n",
    "\n",
    "# 右: グラデーションバー\n",
    "ax2 = axes[1]\n",
    "gradient = np.linspace(0, 255, 256).reshape(1, -1).astype(np.uint8)\n",
    "ax2.imshow(np.repeat(gradient, 50, axis=0), cmap='gray', vmin=0, vmax=255)\n",
    "ax2.set_title('ピクセル値と濃淡の対応', fontsize=12)\n",
    "ax2.set_xlabel('0 (黒) ←――――――→ 255 (白)')\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. なぜCNNが必要か？（特徴量の保持）\n",
    "\n",
    "### 問題：画像データを直接1次元に変換すると情報が失われる\n",
    "\n",
    "画像分類の最終段階では**ソフトマックス関数**を使いますが、この関数は**1次元データ**を必要とします。\n",
    "\n",
    "しかし、画像データは2次元（または3次元）構造を持っており、単純に1次元に変換すると**重要な特徴量**を失う可能性があります。\n",
    "\n",
    "### 特徴量とは？\n",
    "\n",
    "**特徴量**とは、画像が持つ意味のある情報です：\n",
    "- ピクセルの数値そのもの\n",
    "- ピクセル集合同士の**近さ/遠さ**（空間的関係）\n",
    "\n",
    "**例：顔認識**\n",
    "- 人の顔は「目、鼻、口」が特徴\n",
    "- これらは2次元平面で**一定の距離内に集まっている**\n",
    "- 目、鼻、口は**上から下に並んでいる**\n",
    "\n",
    "→ この空間的な位置関係こそが重要な特徴量！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の重要性を示す例\n",
    "# 同じピクセル値でも配置が違えば全く別の画像になる\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# 元の画像（8の形）\n",
    "original = np.array([\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 1, 1, 1, 0]\n",
    "]) * 255\n",
    "\n",
    "# 1次元に変換して再配置（ランダム）\n",
    "flat = original.flatten()\n",
    "np.random.seed(42)\n",
    "shuffled = flat.copy()\n",
    "np.random.shuffle(shuffled)\n",
    "randomized = shuffled.reshape(5, 5)\n",
    "\n",
    "axes[0].imshow(original, cmap='gray', vmin=0, vmax=255)\n",
    "axes[0].set_title('元の画像「8」\\n（空間構造あり）', fontsize=11)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].bar(range(25), flat, color='steelblue')\n",
    "axes[1].set_title('1次元に変換\\n（空間情報が失われる）', fontsize=11)\n",
    "axes[1].set_xlabel('インデックス')\n",
    "axes[1].set_ylabel('ピクセル値')\n",
    "\n",
    "axes[2].imshow(randomized, cmap='gray', vmin=0, vmax=255)\n",
    "axes[2].set_title('シャッフル後\\n（同じピクセル値、違う意味）', fontsize=11)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"→ 単純に1次元化すると、形状（空間的特徴）の情報が失われる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. CNNの全体像\n\nCNNは**形状を保持しながら**特徴を抽出し、最終的に1次元に変換します。\n\n```\n入力層 → [畳み込み層 → プーリング層] × N → 全結合層 → 出力層\n  ↓           ↓              ↓              ↓          ↓\n 画像      特徴抽出       サイズ圧縮      1次元化    確率出力\n(2D/3D)    (2D/3D)        (2D/3D)        (1D)       (1D)\n```\n\n**ポイント**: 畳み込みとプーリングを繰り返すことで、空間構造を保ちながら徐々に特徴を圧縮していく\n\n### 具体的なCNN構成例（図3.4）\n\n手書き数字（0〜9）を分類するCNNの例：\n\n| 層 | フィルタ/サイズ | ストライド | 出力サイズ | 説明 |\n|---|--------------|---------|---------|------|\n| **入力層** | - | - | 24×24 | グレースケール画像 |\n| **畳み込み層①** | 5×5 × 2枚 | 1 | 20×20×2 | 2種類の特徴マップ生成 |\n| **プーリング層①** | 2×2 | 2 | 10×10×2 | サイズを半分に圧縮 |\n| **畳み込み層②** | 3×3 × 8枚 | 1 | 8×8×4 | より複雑な特徴を抽出 |\n| **プーリング層②** | 2×2 | 2 | 4×4×4 | さらに圧縮 |\n| **全結合層** | - | - | 64×1 | 4×4×4=64を1次元化 |\n| **出力層** | ソフトマックス | - | 10 | 0〜9の確率 |\n\n### 用語解説\n- **フィルタ**: 特徴を抽出するための重み行列（カーネルとも呼ぶ）\n- **ストライド**: フィルタを動かす幅\n  - stride=1 → 1ピクセルずつ移動\n  - stride=2 → 2ピクセルずつ移動（出力サイズが半分に）"
  },
  {
   "cell_type": "code",
   "source": "# 図3.4: CNNの具体的な構成例を可視化\n# 各層の出力サイズの変化を追跡\n\ndef calculate_conv_output(input_size, filter_size, stride=1, padding=0):\n    \"\"\"畳み込み層の出力サイズを計算\"\"\"\n    return (input_size - filter_size + 2 * padding) // stride + 1\n\ndef calculate_pool_output(input_size, pool_size, stride):\n    \"\"\"プーリング層の出力サイズを計算\"\"\"\n    return (input_size - pool_size) // stride + 1\n\n# CNNの各層の構成\nlayers = [\n    {\"name\": \"入力層\", \"size\": (24, 24, 1), \"color\": \"lightgray\"},\n    {\"name\": \"畳み込み層①\\n5×5, stride=1\", \"size\": (20, 20, 2), \"color\": \"steelblue\"},\n    {\"name\": \"プーリング層①\\n2×2, stride=2\", \"size\": (10, 10, 2), \"color\": \"lightblue\"},\n    {\"name\": \"畳み込み層②\\n3×3, stride=1\", \"size\": (8, 8, 4), \"color\": \"steelblue\"},\n    {\"name\": \"プーリング層②\\n2×2, stride=2\", \"size\": (4, 4, 4), \"color\": \"lightblue\"},\n    {\"name\": \"全結合層\", \"size\": (64, 1, 1), \"color\": \"orange\"},\n    {\"name\": \"出力層\\nソフトマックス\", \"size\": (10, 1, 1), \"color\": \"green\"},\n]\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nx_pos = 0\nfor i, layer in enumerate(layers):\n    h, w, c = layer[\"size\"]\n    # 表示用のスケーリング\n    display_h = max(h / 3, 0.8)\n    display_w = max(w / 3, 0.8)\n    \n    rect = plt.Rectangle((x_pos, 3 - display_h/2), display_w, display_h, \n                          facecolor=layer[\"color\"], edgecolor='black', linewidth=2)\n    ax.add_patch(rect)\n    \n    # サイズラベル\n    if c > 1:\n        size_text = f\"{h}×{w}×{c}\"\n    else:\n        size_text = f\"{h}×{w}\" if w > 1 else f\"{h}\"\n    ax.text(x_pos + display_w/2, 3 - display_h/2 - 0.3, size_text, \n            ha='center', va='top', fontsize=9)\n    \n    # 層の名前\n    ax.text(x_pos + display_w/2, 3 + display_h/2 + 0.2, layer[\"name\"], \n            ha='center', va='bottom', fontsize=8)\n    \n    # 矢印\n    if i < len(layers) - 1:\n        ax.annotate('', xy=(x_pos + display_w + 0.3, 3), \n                   xytext=(x_pos + display_w + 0.1, 3),\n                   arrowprops=dict(arrowstyle='->', color='black'))\n    \n    x_pos += display_w + 0.5\n\nax.set_xlim(-0.5, x_pos)\nax.set_ylim(0, 6)\nax.set_aspect('equal')\nax.axis('off')\nax.set_title('図3.4: CNNの構成例（手書き数字分類）', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# 出力サイズの計算を確認\nprint(\"出力サイズの計算:\")\nprint(f\"  入力: 24×24\")\nprint(f\"  畳み込み①: (24-5)/1+1 = 20 → 20×20×2\")\nprint(f\"  プーリング①: (20-2)/2+1 = 10 → 10×10×2\")\nprint(f\"  畳み込み②: (10-3)/1+1 = 8 → 8×8×4\")\nprint(f\"  プーリング②: (8-2)/2+1 = 4 → 4×4×4\")\nprint(f\"  全結合: 4×4×4 = 64 → 64×1\")\nprint(f\"  出力: 10クラス（0〜9）\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 各層の役割\n",
    "\n",
    "| 層 | 処理内容 | 入出力 |\n",
    "|---|---------|-------|\n",
    "| **入力層** | 画像データを受け取る | → 2D/3D |\n",
    "| **畳み込み層** | フィルタで積和演算、特徴を抽出 | 2D/3D → 特徴マップ |\n",
    "| **プーリング層** | 特徴マップのサイズを縮小（データ量削減） | 特徴マップ → 小さい特徴マップ |\n",
    "| **全結合層** | すべての特徴量を1次元に配列 | 2D/3D → 1D |\n",
    "| **出力層** | ソフトマックス関数で確率を出力 | 1D → 分類結果 |\n",
    "\n",
    "### CNNの名前の由来\n",
    "- **C**NN = **C**onvolutional Neural Network\n",
    "- **Convolutional** = 畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNの処理フローを視覚化\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# 1. 入力画像\n",
    "input_img = np.random.randint(0, 255, (28, 28))\n",
    "axes[0].imshow(input_img, cmap='gray')\n",
    "axes[0].set_title('入力層\\n(28×28)', fontsize=10)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 2. 畳み込み後\n",
    "conv1 = np.random.randint(0, 255, (24, 24))\n",
    "axes[1].imshow(conv1, cmap='Blues')\n",
    "axes[1].set_title('畳み込み層\\n(24×24)\\n特徴抽出', fontsize=10)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 3. プーリング後\n",
    "pool1 = np.random.randint(0, 255, (12, 12))\n",
    "axes[2].imshow(pool1, cmap='Blues')\n",
    "axes[2].set_title('プーリング層\\n(12×12)\\nサイズ圧縮', fontsize=10)\n",
    "axes[2].axis('off')\n",
    "\n",
    "# 4. 全結合層\n",
    "fc = np.random.rand(1, 50)\n",
    "axes[3].imshow(fc, cmap='Oranges', aspect='auto')\n",
    "axes[3].set_title('全結合層\\n(1×N)\\n1次元化', fontsize=10)\n",
    "axes[3].axis('off')\n",
    "\n",
    "# 5. 出力層\n",
    "output = np.zeros(10)\n",
    "output[8] = 0.95  # \"8\"の確率が高い\n",
    "axes[4].bar(range(10), output, color='green')\n",
    "axes[4].set_title('出力層\\n(確率分布)\\nソフトマックス', fontsize=10)\n",
    "axes[4].set_xticks(range(10))\n",
    "axes[4].set_xlabel('クラス (0-9)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 手書き数字分類（MNIST）\n",
    "\n",
    "MNISTデータセット：28×28ピクセルのグレースケール手書き数字（0〜9）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTデータセットの読み込み\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# データの前処理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST用の正規化\n",
    "])\n",
    "\n",
    "# データセットのダウンロード\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"訓練データ: {len(train_dataset)} 枚\")\n",
    "print(f\"テストデータ: {len(test_dataset)} 枚\")\n",
    "print(f\"画像サイズ: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像の表示\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'ラベル: {label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNISTデータセットのサンプル', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "1. **画像データ**はピクセルの2D/3D配列で、0〜255の値で濃淡/色を表現\n",
    "2. **特徴量**（空間的な位置関係）を保持することが画像認識で重要\n",
    "3. **CNN**は畳み込み→プーリングを繰り返し、形状を保ちながら特徴を抽出\n",
    "4. 最終的に**全結合層**で1次元化し、**ソフトマックス**で確率を出力\n",
    "\n",
    "## 次のステップ\n",
    "\n",
    "次のノートブックでは、**畳み込み演算**の数理について詳しく学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}