{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 第2章 商品推薦を実現する数理\n\nこのNotebookでは、レコメンドシステムを実現するための数理的基礎を学びます。\n\n**キーワード**：協調フィルタリング、行列因子分解、最小二乗法、勾配降下法、評価値行列、コサイン類似度、指示関数、損失関数、内積、三角関数、ベクトル、微分、偏微分、行列\n\n## 目次\n- 2-1 はじめに\n- 2-2 商品の評価を数理的に表現する ー評価値行列ー\n- 2-3 評価値の予測を数理モデルで実現する ー協調フィルタリングと行列因子分解ー\n- 2-4 ユーザー同士の類似度で予測値を推計する ー内積の定理とコサイン類似度ー\n- 2-5 コサイン類似度の意味を考える ー三角関数ー\n- 2-6 コサイン類似度を複数のアイテムに適用する ー多次元への拡張ー\n- 2-7 コサイン類似度を改良する ー中心化ー\n- 2-8 コサイン類似度を計算する ー指示関数ー\n- 2-9 欠損値を推計する数理モデルを設計し、計算を実行する\n- 2-10 アイテム同士の類似度で予測値を推計する\n- 2-11 ユーザー目線で数理モデルを再考する ーセレンディピティー\n- 2-12 課題解決のために数理モデルを変更する ー行列因子分解ー\n- 2-13 評価値の推計を最適化問題に置き換える ー残差行列と誤差ー"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語フォントのインストール（Google Colab用）\n",
    "!pip install japanize-matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "# グラフの設定\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-1 はじめに\n",
    "\n",
    "私たちの生活は**レコメンド**に溢れています。\n",
    "\n",
    "- オンラインショッピングの「おすすめの商品」\n",
    "- YouTubeの「あなたへのおすすめ動画」\n",
    "- SNSの「興味のある投稿」\n",
    "\n",
    "レコメンドは、レコメンドのために作られた**数理モデル**が膨大なデータを処理することで実現します。\n",
    "\n",
    "本章では、レコメンドを実現する基礎的な数学的手法に焦点を当てて解説します。具体的には、**協調フィルタリング**と**行列因子分解**と呼ばれる数理的手法を中心に解説し、いかにして数理モデルが私たちの「好み」を理解し、購入する可能性が高い商品やコンテンツをレコメンドしているのかを解き明かします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-2 商品の評価を数理的に表現する ー評価値行列ー\n",
    "\n",
    "レコメンドを数理モデルで実現するために、まず**評価値行列**を作成します。\n",
    "\n",
    "## ベクトルによる表現\n",
    "\n",
    "ベクトルは数値の順序付きリストです。\n",
    "\n",
    "$$\\vec{r} = \\begin{pmatrix} r_1 \\\\ r_2 \\\\ \\vdots \\\\ r_n \\end{pmatrix}, \\quad \\vec{r}^T = (r_1, r_2, \\cdots, r_n)$$\n",
    "\n",
    "- 左側の式は縦の「列方向」に数値が並んでいるので**列ベクトル**\n",
    "- 右側の式は横の「行方向」に数値が並んでいるので**行ベクトル**\n",
    "- $^T$ は**転置**を表す記号で、行と列の方向を変える操作を指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーザーごとの評価値をベクトルで表現\n",
    "# 図2.3の例：4人のユーザーが4つのアイテムを評価\n",
    "# 0は欠損値（未評価）を表す\n",
    "\n",
    "# 行ベクトル表現\n",
    "R_u1 = np.array([2, 3, 0, 5])  # user1の評価\n",
    "R_u2 = np.array([2, 5, 0, 5])  # user2の評価\n",
    "R_u3 = np.array([0, 3, 4, 4])  # user3の評価\n",
    "R_u4 = np.array([4, 2, 3, 0])  # user4の評価\n",
    "\n",
    "print(\"=== ユーザーごとの評価ベクトル（行ベクトル） ===\")\n",
    "print(f\"R_u1 = {R_u1}\")\n",
    "print(f\"R_u2 = {R_u2}\")\n",
    "print(f\"R_u3 = {R_u3}\")\n",
    "print(f\"R_u4 = {R_u4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列ベクトル表現（アイテムごとの評価）\n",
    "R_i1 = np.array([[2], [2], [0], [4]])  # item1の評価\n",
    "R_i2 = np.array([[3], [5], [3], [2]])  # item2の評価\n",
    "R_i3 = np.array([[0], [0], [4], [3]])  # item3の評価\n",
    "R_i4 = np.array([[5], [5], [4], [0]])  # item4の評価\n",
    "\n",
    "print(\"=== アイテムごとの評価ベクトル（列ベクトル） ===\")\n",
    "print(f\"R_i1 = {R_i1.flatten()}\")\n",
    "print(f\"R_i2 = {R_i2.flatten()}\")\n",
    "print(f\"R_i3 = {R_i3.flatten()}\")\n",
    "print(f\"R_i4 = {R_i4.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価値行列 R\n",
    "\n",
    "行ベクトル $R_{u_1} \\sim R_{u_4}$ を列方向（縦）に並べると、評価値を1つにまとめた**行列**で表現できます。これを**評価値行列**と呼び、$R$と表記します。\n",
    "\n",
    "$$R = \\begin{pmatrix} R_{u_1} \\\\ R_{u_2} \\\\ R_{u_3} \\\\ R_{u_4} \\end{pmatrix} = \\begin{pmatrix} 2 & 3 & 0 & 5 \\\\ 2 & 5 & 0 & 5 \\\\ 0 & 3 & 4 & 4 \\\\ 4 & 2 & 3 & 0 \\end{pmatrix}$$\n",
    "\n",
    "- $R$ は Rating（評価）の頭文字\n",
    "- 0 は未評価、つまり**欠損値**を示す\n",
    "- user1のitem1に対する評価値を $r_{1,1}$ と表記する\n",
    "\n",
    "一般化すると、ユーザーの総数を $U$、アイテムの総数を $I$ とすると：\n",
    "\n",
    "$$R = \\begin{pmatrix} r_{1,1} & r_{1,2} & \\cdots & r_{1,I} \\\\ r_{2,1} & r_{2,2} & \\cdots & r_{2,I} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ r_{U,1} & r_{U,2} & \\cdots & r_{U,I} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価値行列の作成\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],\n",
    "    [2, 5, 0, 5],\n",
    "    [0, 3, 4, 4],\n",
    "    [4, 2, 3, 0]\n",
    "])\n",
    "\n",
    "print(\"=== 評価値行列 R ===\")\n",
    "print(R)\n",
    "print(f\"\\n行列のサイズ: {R.shape[0]}ユーザー × {R.shape[1]}アイテム\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価値行列の可視化\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# ヒートマップを作成（0は欠損値として特別扱い）\n",
    "masked_R = np.ma.masked_where(R == 0, R)\n",
    "im = ax.imshow(masked_R, cmap='YlOrRd', vmin=1, vmax=5)\n",
    "\n",
    "# 欠損値（0）をグレーで表示\n",
    "ax.imshow(np.where(R == 0, 1, np.nan), cmap='gray', vmin=0, vmax=1, alpha=0.3)\n",
    "\n",
    "# 軸ラベル\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\n",
    "ax.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "\n",
    "# 各セルに値を表示\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if R[i, j] == 0:\n",
    "            text = ax.text(j, i, '−', ha='center', va='center', color='gray', fontsize=14)\n",
    "        else:\n",
    "            text = ax.text(j, i, R[i, j], ha='center', va='center', color='black', fontsize=14)\n",
    "\n",
    "ax.set_title('評価値行列 R（グレー: 欠損値）')\n",
    "plt.colorbar(im, label='評価値')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-3 評価値の予測を数理モデルで実現する ー協調フィルタリングと行列因子分解ー\n",
    "\n",
    "## レコメンドの前提条件\n",
    "\n",
    "- ユーザーは、購入したすべての商品に評価値を付与している\n",
    "- 逆に、ユーザーは購入していない商品に評価値を付与していない\n",
    "- レコメンドする商品は、そのユーザーが未購入の商品（評価値が欠損している商品）に限る\n",
    "\n",
    "つまり、**欠損部分の評価値を予測値として算出できれば、「もし購入したらいくらの評価を付けるか」を推定できる**。これがレコメンドの有効な判断指標となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レコメンドの概念図\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 左：評価値がある部分とない部分\n",
    "ax1 = axes[0]\n",
    "colors = np.where(R == 0, 0.3, 0.8)\n",
    "ax1.imshow(colors, cmap='Blues', vmin=0, vmax=1)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if R[i, j] == 0:\n",
    "            ax1.text(j, i, '?', ha='center', va='center', fontsize=16, color='red', fontweight='bold')\n",
    "        else:\n",
    "            ax1.text(j, i, str(R[i, j]), ha='center', va='center', fontsize=14)\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\n",
    "ax1.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "ax1.set_title('評価値行列（? = 欠損値 = レコメンド対象）')\n",
    "\n",
    "# 右：予測値の例\n",
    "ax2 = axes[1]\n",
    "R_predicted = R.copy().astype(float)\n",
    "R_predicted[0, 2] = 4.2  # user1のitem3を予測\n",
    "R_predicted[1, 2] = 4.5  # user2のitem3を予測\n",
    "R_predicted[2, 0] = 2.1  # user3のitem1を予測\n",
    "R_predicted[3, 3] = 3.8  # user4のitem4を予測\n",
    "\n",
    "im2 = ax2.imshow(R_predicted, cmap='YlOrRd', vmin=1, vmax=5)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if R[i, j] == 0:\n",
    "            ax2.text(j, i, f'{R_predicted[i,j]:.1f}', ha='center', va='center', \n",
    "                    fontsize=12, color='blue', fontweight='bold')\n",
    "        else:\n",
    "            ax2.text(j, i, str(int(R_predicted[i, j])), ha='center', va='center', fontsize=14)\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "ax2.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\n",
    "ax2.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "ax2.set_title('予測後（青字 = 予測値）')\n",
    "plt.colorbar(im2, ax=ax2, label='評価値')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"例：user1のitem3の予測値が4.2なら、item3をuser1にレコメンドすべき！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測値の推計方法：3つのアプローチ\n",
    "\n",
    "| 手法 | 説明 | 軸 |\n",
    "|:---|:---|:---|\n",
    "| ①userを軸とした予測値の算出 | 各itemの評価が似ているuserを選び、そのuserの評価値を用いて予測値を算出 | user |\n",
    "| ②itemを軸とした予測値の算出 | 各userの評価が似ているitemを選び、そのitemの評価値を用いて予測値を算出 | item |\n",
    "| ③行列因子分解（MF） | 評価値行列を分解した潜在因子行列を解析的に求めることで、予測値を算出 | − |\n",
    "\n",
    "①と②は**協調フィルタリング**という手法に分類されます。\n",
    "③は協調フィルタリングとは異なる手法で、評価値行列を**潜在因子行列**と呼ばれる行列に分解して欠損値を推定するアプローチです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つのアプローチの概念図\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ①userを軸とした予測\n",
    "ax1 = axes[0]\n",
    "ax1.text(0.5, 0.9, '①userを軸とした予測値の算出', ha='center', fontsize=11, fontweight='bold', transform=ax1.transAxes)\n",
    "ax1.text(0.5, 0.7, '似ているユーザーの評価値を利用', ha='center', fontsize=10, transform=ax1.transAxes)\n",
    "ax1.annotate('', xy=(0.7, 0.4), xytext=(0.3, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax1.text(0.2, 0.4, 'user1', fontsize=10, ha='center', va='center')\n",
    "ax1.text(0.8, 0.4, 'user2\\n(類似)', fontsize=10, ha='center', va='center', color='blue')\n",
    "ax1.text(0.5, 0.2, '→ user2の評価を参考にuser1の欠損を予測', ha='center', fontsize=9, transform=ax1.transAxes)\n",
    "ax1.axis('off')\n",
    "\n",
    "# ②itemを軸とした予測\n",
    "ax2 = axes[1]\n",
    "ax2.text(0.5, 0.9, '②itemを軸とした予測値の算出', ha='center', fontsize=11, fontweight='bold', transform=ax2.transAxes)\n",
    "ax2.text(0.5, 0.7, '似ているアイテムの評価値を利用', ha='center', fontsize=10, transform=ax2.transAxes)\n",
    "ax2.annotate('', xy=(0.7, 0.4), xytext=(0.3, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "ax2.text(0.2, 0.4, 'item1', fontsize=10, ha='center', va='center')\n",
    "ax2.text(0.8, 0.4, 'item2\\n(類似)', fontsize=10, ha='center', va='center', color='green')\n",
    "ax2.text(0.5, 0.2, '→ item2の評価を参考にitem1の欠損を予測', ha='center', fontsize=9, transform=ax2.transAxes)\n",
    "ax2.axis('off')\n",
    "\n",
    "# ③行列因子分解\n",
    "ax3 = axes[2]\n",
    "ax3.text(0.5, 0.9, '③行列因子分解（MF）', ha='center', fontsize=11, fontweight='bold', transform=ax3.transAxes)\n",
    "ax3.text(0.5, 0.7, 'R ≈ P × Q', ha='center', fontsize=12, transform=ax3.transAxes)\n",
    "ax3.text(0.15, 0.4, 'R', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax3.text(0.32, 0.4, '≈', fontsize=14, ha='center')\n",
    "ax3.text(0.5, 0.4, 'P', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax3.text(0.62, 0.4, '×', fontsize=14, ha='center')\n",
    "ax3.text(0.75, 0.4, 'Q', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax3.text(0.5, 0.2, '→ 潜在因子行列に分解して欠損を推定', ha='center', fontsize=9, transform=ax3.transAxes)\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-4 ユーザー同士の類似度で予測値を推計する ー内積の定理とコサイン類似度ー\n",
    "\n",
    "まずは「①userを軸とした予測値の算出」について解説します。\n",
    "\n",
    "この手法では、各itemの評価が似ているuserを選び、そのuserの評価値を用いて予測値を算出します。\n",
    "このとき、「各itemの評価が似ているuserを選ぶ」という処理を実現するために、**コサイン類似度**という手法を採用します。\n",
    "\n",
    "## 基礎となる数理的手法と情報工学的アプローチ\n",
    "\n",
    "| 基礎となる数理的手法 | 情報工学的アプローチ |\n",
    "|:---|:---|\n",
    "| 内積の定理：$\\vec{a} \\cdot \\vec{b} = |\\vec{a}||\\vec{b}|\\cos\\theta$ | コサイン類似度：$\\cos(a, b) = \\frac{\\sum_{k=1}^{n} a_k b_k}{\\sqrt{\\sum_{k=1}^{n} a_k^2} \\sqrt{\\sum_{k=1}^{n} b_k^2}}$ |\n",
    "| ベクトルの大きさ（三平方の定理）：$|\\vec{a}| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2}$ | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 座標空間での表現\n",
    "\n",
    "例として、userが6人、itemが2個だけの状況を想定し、以下の評価値行列が作成されたとしましょう。\n",
    "\n",
    "$$R = \\begin{pmatrix} 2 & 3 \\\\ 2 & 5 \\\\ 0 & 3 \\\\ 4 & 2 \\\\ 4 & 4 \\\\ 3 & 0 \\end{pmatrix}$$\n",
    "\n",
    "この評価値行列を**座標空間**で示すと、ユーザー同士の類似度合いが視覚的に把握できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6人のユーザー、2つのアイテム\n",
    "R_simple = np.array([\n",
    "    [2, 3],  # user1\n",
    "    [2, 5],  # user2\n",
    "    [0, 3],  # user3 (item1未評価)\n",
    "    [4, 2],  # user4\n",
    "    [4, 4],  # user5\n",
    "    [3, 0]   # user6 (item2未評価)\n",
    "])\n",
    "\n",
    "print(\"評価値行列 R:\")\n",
    "print(R_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図2.10: 座標空間でのプロット\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "users = ['user1', 'user2', 'user3', 'user4', 'user5', 'user6']\n",
    "colors = ['blue', 'blue', 'gray', 'blue', 'blue', 'gray']\n",
    "\n",
    "for i, (user, color) in enumerate(zip(users, colors)):\n",
    "    x, y = R_simple[i]\n",
    "    if x == 0 or y == 0:\n",
    "        ax.scatter(x, y, s=100, c='gray', alpha=0.5, zorder=5)\n",
    "        ax.annotate(user, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=10, color='gray')\n",
    "    else:\n",
    "        ax.scatter(x, y, s=100, c='blue', zorder=5)\n",
    "        ax.annotate(user, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlim(-0.5, 5.5)\n",
    "ax.set_ylim(-0.5, 5.5)\n",
    "ax.set_xlabel('item1への評価値', fontsize=12)\n",
    "ax.set_ylabel('item2への評価値', fontsize=12)\n",
    "ax.set_title('図2.10: 座標空間でのユーザー分布', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(-0.3, 0, '評価\\nなし', fontsize=9, va='center', color='gray')\n",
    "ax.text(0, -0.3, '評価なし', fontsize=9, ha='center', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトルによる表現\n",
    "\n",
    "ここで、目的を「user1に似ているユーザーが誰かを特定すること」とし、コサイン類似度によって類似ユーザーを特定します。\n",
    "\n",
    "考えやすいように、考察対象をuser1, user2, user4に絞り、各userをベクトルで表現します。\n",
    "\n",
    "$$\\vec{u}_1 = (2, 3), \\quad \\vec{u}_2 = (2, 5), \\quad \\vec{u}_4 = (4, 2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図2.11: ベクトルとしての表現\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "u1 = np.array([2, 3])\n",
    "u2 = np.array([2, 5])\n",
    "u4 = np.array([4, 2])\n",
    "\n",
    "ax.quiver(0, 0, u1[0], u1[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='blue', width=0.02, label=f'$\\\\vec{{u}}_1$ = (2, 3)')\n",
    "ax.quiver(0, 0, u2[0], u2[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='green', width=0.02, label=f'$\\\\vec{{u}}_2$ = (2, 5)')\n",
    "ax.quiver(0, 0, u4[0], u4[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='red', width=0.02, label=f'$\\\\vec{{u}}_4$ = (4, 2)')\n",
    "\n",
    "ax.plot(u1[0], u1[1], 'bo', markersize=8)\n",
    "ax.plot(u2[0], u2[1], 'go', markersize=8)\n",
    "ax.plot(u4[0], u4[1], 'ro', markersize=8)\n",
    "\n",
    "ax.annotate(f'({u1[0]},{u1[1]})\\nuser1', (u1[0], u1[1]), xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "ax.annotate(f'({u2[0]},{u2[1]})\\nuser2', (u2[0], u2[1]), xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "ax.annotate(f'({u4[0]},{u4[1]})\\nuser4', (u4[0], u4[1]), xytext=(10, -15), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlim(-0.5, 5.5)\n",
    "ax.set_ylim(-0.5, 5.5)\n",
    "ax.set_xlabel('item1', fontsize=12)\n",
    "ax.set_ylabel('item2', fontsize=12)\n",
    "ax.set_title('図2.11: user1, user2, user4をベクトルで表現', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"図から、user1に似ているのはuser2だと言えそうです。\")\n",
    "print(\"なぜなら、ベクトルを示す矢印のuser1とuser2が作る角度がuser1とuser4が作る角度よりも小さいからです。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内積の定理\n",
    "\n",
    "角度を「目」で見て類似度を評価することはできますが、コンピュータは「目」で判断することはできず、**数値**でしか判断できません。\n",
    "\n",
    "そこで、この角度を**定量化**します。このときに便利な数理的手法が高校数学で学習した**内積の定理**です。\n",
    "\n",
    "2つのベクトル $\\vec{a}$, $\\vec{b}$ があり、$\\vec{a}$, $\\vec{b}$ のなす角を $\\theta$（theta：シータ）とします。\n",
    "\n",
    "すると、**内積**は式 (2-3) のように定義されます。\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = |\\vec{a}||\\vec{b}|\\cos\\theta \\quad (2\\text{-}3)$$\n",
    "\n",
    "## cos θ の計算\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} \\quad (2\\text{-}4)$$\n",
    "\n",
    "### 分子：内積 $\\vec{a} \\cdot \\vec{b}$ の計算\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = a_1 b_1 + a_2 b_2 \\quad (2\\text{-}6)$$\n",
    "\n",
    "### 分母：ベクトルの大きさ（ノルム）の計算\n",
    "\n",
    "$$|\\vec{a}| = \\sqrt{a_1^2 + a_2^2} \\quad (2\\text{-}10)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コサイン類似度の計算（基本版）\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    2つのベクトルのコサイン類似度を計算する\n",
    "    式: cos(a, b) = (a · b) / (|a| × |b|)\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(\"cosine_similarity関数を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user1, user2, user4のコサイン類似度を計算\n",
    "u1 = np.array([2, 3])\n",
    "u2 = np.array([2, 5])\n",
    "u4 = np.array([4, 2])\n",
    "\n",
    "print(\"=== コサイン類似度の計算 ===\")\n",
    "print(f\"u1 = {tuple(u1)}\")\n",
    "print(f\"u2 = {tuple(u2)}\")\n",
    "print(f\"u4 = {tuple(u4)}\")\n",
    "print()\n",
    "\n",
    "cos_12 = cosine_similarity(u1, u2)\n",
    "cos_14 = cosine_similarity(u1, u4)\n",
    "\n",
    "print(f\"cos(u1, u2) = {cos_12:.4f}\")\n",
    "print(f\"cos(u1, u4) = {cos_14:.4f}\")\n",
    "print()\n",
    "print(f\"結論: cos(u1, u2) = {cos_12:.3f} > cos(u1, u4) = {cos_14:.3f}\")\n",
    "print(\"→ user2のほうがuser1に似ている\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-5 コサイン類似度の意味を考える ー三角関数ー\n",
    "\n",
    "## 単位円を用いた cos θ の定義\n",
    "\n",
    "**単位円**とは原点を中心とした半径1の円です。\n",
    "\n",
    "- 単位円上に点 $P$ を取り、原点 $O$ を中心に $x$ 軸の正の部分から反時計回りに $\\theta$ だけ回転させた線分 $OP$ について\n",
    "- $P$ の $x$ 座標を $\\cos\\theta$ と定義します\n",
    "- $P$ の $y$ 座標は $\\sin\\theta$ と定義します\n",
    "\n",
    "## θ と cos θ の関係\n",
    "\n",
    "$0 \\leq \\theta \\leq 180°$ のとき、$\\theta$ の値が大きくなるほど $\\cos\\theta$ の値が小さくなります。\n",
    "\n",
    "$$-1 \\leq \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} \\leq 1 \\quad (2\\text{-}7)$$\n",
    "\n",
    "だから、計算結果が**1に近いほど**、2つのベクトルは「**似ている**」と言えるのです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# θ と cos θ の関係グラフ\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "theta_deg = np.linspace(0, 180, 100)\n",
    "theta_rad = np.radians(theta_deg)\n",
    "cos_values = np.cos(theta_rad)\n",
    "\n",
    "ax.plot(theta_deg, cos_values, 'b-', linewidth=2)\n",
    "\n",
    "important_points = [(0, 1, '同じ方向\\n(最も類似)'), \n",
    "                    (90, 0, '直交\\n(無関係)'), \n",
    "                    (180, -1, '反対方向\\n(最も非類似)')]\n",
    "\n",
    "for angle, cos_val, label in important_points:\n",
    "    ax.plot(angle, cos_val, 'ro', markersize=10)\n",
    "    ax.annotate(f'{label}\\n({angle}°, {cos_val})', xy=(angle, cos_val),\n",
    "                xytext=(angle+15, cos_val), fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax.axhline(y=0, color='k', linewidth=0.5, linestyle='--')\n",
    "ax.fill_between(theta_deg, 0, cos_values, where=(cos_values > 0), alpha=0.3, color='green', label='類似 (cos θ > 0)')\n",
    "ax.fill_between(theta_deg, 0, cos_values, where=(cos_values < 0), alpha=0.3, color='red', label='非類似 (cos θ < 0)')\n",
    "\n",
    "ax.set_xlabel('角度 θ (度)', fontsize=12)\n",
    "ax.set_ylabel('cos θ', fontsize=12)\n",
    "ax.set_title('角度θとcos θの関係', fontsize=14)\n",
    "ax.set_xticks([0, 30, 60, 90, 120, 150, 180])\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-6 コサイン類似度を複数のアイテムに適用する ー多次元への拡張ー\n",
    "\n",
    "## n次元への拡張\n",
    "\n",
    "アイテム数が $n$ 個になった場合：\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\sum_{k=1}^{n} a_k b_k \\quad (2\\text{-}9)$$\n",
    "\n",
    "$$|\\vec{a}| = \\sqrt{\\sum_{k=1}^{n} a_k^2} \\quad (2\\text{-}12)$$\n",
    "\n",
    "## 一般化されたコサイン類似度の公式\n",
    "\n",
    "$$\\cos(a, b) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} = \\frac{\\sum_{k=1}^{n} a_k b_k}{\\sqrt{\\sum_{k=1}^{n} a_k^2} \\sqrt{\\sum_{k=1}^{n} b_k^2}} \\quad (2\\text{-}13)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-7 コサイン類似度を改良する ー中心化ー\n",
    "\n",
    "## 問題点：評価が「逆」のユーザー\n",
    "\n",
    "例えば、item1〜5について評価が「逆」になっている2人のuserの評価値ベクトルを考えます。\n",
    "\n",
    "$$\\vec{u}_A = (1, 2, 3, 4, 5), \\quad \\vec{u}_B = (5, 4, 3, 2, 1)$$\n",
    "\n",
    "両者の評価は真逆なので、コサイン類似度は低く算出されて欲しいところです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価が「逆」のユーザーでコサイン類似度を計算\n",
    "u_A = np.array([1, 2, 3, 4, 5])\n",
    "u_B = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "print(\"=== 評価が「逆」のユーザー ===\")\n",
    "print(f\"u_A = {u_A}\")\n",
    "print(f\"u_B = {u_B}\")\n",
    "print()\n",
    "\n",
    "# 内積の計算\n",
    "dot_AB = np.dot(u_A, u_B)\n",
    "print(f\"内積: u_A · u_B = 1×5 + 2×4 + 3×3 + 4×2 + 5×1 = {dot_AB}\")\n",
    "\n",
    "# ノルムの計算\n",
    "norm_A = np.linalg.norm(u_A)\n",
    "norm_B = np.linalg.norm(u_B)\n",
    "print(f\"|u_A| = √(1² + 2² + 3² + 4² + 5²) = √{sum(u_A**2)} = {norm_A:.4f}\")\n",
    "print(f\"|u_B| = √(5² + 4² + 3² + 2² + 1²) = √{sum(u_B**2)} = {norm_B:.4f}\")\n",
    "\n",
    "# コサイン類似度\n",
    "cos_AB = cosine_similarity(u_A, u_B)\n",
    "print(f\"\\ncos(u_A, u_B) = {dot_AB} / ({norm_A:.4f} × {norm_B:.4f}) = {cos_AB:.3f}\")\n",
    "print()\n",
    "print(\"問題：評価が真逆なのに、コサイン類似度が0.636と高い！\")\n",
    "print(\"これは最大値1に近く、「似ている」と判断されてしまう。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解決策：中心化（Centering）\n",
    "\n",
    "この評価値の差を緩和します。具体的には、userごとに各アイテムへの評価値の**平均値**を算出し、その平均値を各評価値から引きます。\n",
    "\n",
    "この処理を**中心化**と言います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図2.21: 中心化の可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "items = ['item1', 'item2', 'item3', 'item4', 'item5']\n",
    "\n",
    "# 左：中心化前\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(items))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, u_A, width, label='user A', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, u_B, width, label='user B', color='orange', alpha=0.7)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(items)\n",
    "ax1.set_ylabel('評価値')\n",
    "ax1.set_title('中心化前')\n",
    "ax1.legend()\n",
    "ax1.axhline(y=np.mean(u_A), color='blue', linestyle='--', alpha=0.5, label=f'A平均={np.mean(u_A)}')\n",
    "ax1.axhline(y=np.mean(u_B), color='orange', linestyle='--', alpha=0.5, label=f'B平均={np.mean(u_B)}')\n",
    "ax1.set_ylim(0, 6)\n",
    "\n",
    "# 中心化\n",
    "u_A_centered = u_A - np.mean(u_A)\n",
    "u_B_centered = u_B - np.mean(u_B)\n",
    "\n",
    "# 右：中心化後\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, u_A_centered, width, label='user A (中心化後)', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, u_B_centered, width, label='user B (中心化後)', color='orange', alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(items)\n",
    "ax2.set_ylabel('評価値（中心化後）')\n",
    "ax2.set_title('中心化後')\n",
    "ax2.legend()\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax2.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle('図2.21: ユーザーごとに評価値の平均値を算出し、各評価値から減算する', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"u_A 平均値: {np.mean(u_A)}\")\n",
    "print(f\"u_A 中心化後: {u_A_centered}\")\n",
    "print(f\"u_B 平均値: {np.mean(u_B)}\")\n",
    "print(f\"u_B 中心化後: {u_B_centered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心化後のコサイン類似度を計算\n",
    "print(\"=== 中心化後のコサイン類似度計算 ===\")\n",
    "print(f\"u_A (中心化後) = {u_A_centered}\")\n",
    "print(f\"u_B (中心化後) = {u_B_centered}\")\n",
    "print()\n",
    "\n",
    "# 内積\n",
    "dot_AB_centered = np.dot(u_A_centered, u_B_centered)\n",
    "print(f\"内積: {u_A_centered[0]:.0f}×{u_B_centered[0]:.0f} + {u_A_centered[1]:.0f}×{u_B_centered[1]:.0f} + ... = {dot_AB_centered}\")\n",
    "\n",
    "# ノルム\n",
    "norm_A_centered = np.linalg.norm(u_A_centered)\n",
    "norm_B_centered = np.linalg.norm(u_B_centered)\n",
    "print(f\"|u_A| = √{sum(u_A_centered**2):.0f} = {norm_A_centered:.4f}\")\n",
    "print(f\"|u_B| = √{sum(u_B_centered**2):.0f} = {norm_B_centered:.4f}\")\n",
    "\n",
    "# コサイン類似度\n",
    "cos_AB_centered = cosine_similarity(u_A_centered, u_B_centered)\n",
    "print(f\"\\ncos(u_A, u_B) = {dot_AB_centered} / ({norm_A_centered:.4f} × {norm_B_centered:.4f}) = {cos_AB_centered:.3f}\")\n",
    "print()\n",
    "print(\"結果：コサイン類似度が -1 になりました！\")\n",
    "print(\"これは最小値で、「評価が真逆」という判断と一致します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改良したコサイン類似度の公式\n",
    "\n",
    "各要素から平均値を引いた計算が実行されればよいので、改良したモデルは式 (2-14) のようになります。\n",
    "\n",
    "$\\bar{a}$, $\\bar{b}$ は、それぞれ $\\vec{a}$, $\\vec{b}$ の平均評価値を表しています。\n",
    "\n",
    "$$\\cos(a, b) = \\frac{\\sum_{k=1}^{n} (a_k - \\bar{a})(b_k - \\bar{b})}{\\sqrt{\\sum_{k=1}^{n} (a_k - \\bar{a})^2} \\sqrt{\\sum_{k=1}^{n} (b_k - \\bar{b})^2}} \\quad (2\\text{-}14)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心化付きコサイン類似度関数\n",
    "def cosine_similarity_centered(a, b, mask_a=None, mask_b=None):\n",
    "    \"\"\"\n",
    "    中心化を行ったコサイン類似度を計算する\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a, b : array-like\n",
    "        評価ベクトル\n",
    "    mask_a, mask_b : array-like, optional\n",
    "        評価が存在する要素を示すマスク（Trueの要素のみ平均計算に使用）\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        中心化されたコサイン類似度\n",
    "    \"\"\"\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    \n",
    "    # マスクが指定されていない場合は、0以外を有効とする\n",
    "    if mask_a is None:\n",
    "        mask_a = (a != 0)\n",
    "    if mask_b is None:\n",
    "        mask_b = (b != 0)\n",
    "    \n",
    "    # 共通して評価しているアイテムのみ使用\n",
    "    common_mask = mask_a & mask_b\n",
    "    \n",
    "    if np.sum(common_mask) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 平均値を計算（共通アイテムのみ）\n",
    "    mean_a = np.mean(a[common_mask])\n",
    "    mean_b = np.mean(b[common_mask])\n",
    "    \n",
    "    # 中心化\n",
    "    a_centered = a[common_mask] - mean_a\n",
    "    b_centered = b[common_mask] - mean_b\n",
    "    \n",
    "    # コサイン類似度を計算\n",
    "    dot_product = np.dot(a_centered, b_centered)\n",
    "    norm_a = np.linalg.norm(a_centered)\n",
    "    norm_b = np.linalg.norm(b_centered)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(\"cosine_similarity_centered関数を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-8 コサイン類似度を計算する ー指示関数ー\n",
    "\n",
    "## 指示関数 $\\delta_{u,i}$\n",
    "\n",
    "「評価値が与えられている要素のみを計算対象とする」という制約を与えています。しかし、コサイン類似度に用いられている $\\sum$ は、都合よく「これは足す」「これは足さない」という処理をしてくれるわけではありません。\n",
    "\n",
    "このような条件2の操作を実現するために、**指示関数** $\\delta_{u,i}$ を式 (2-15) のように定義します。$u$ はユーザーを示す行、$i$ はアイテムを示す列を示しています。\n",
    "\n",
    "$$\\delta_{u,i} = \\begin{cases} 1 & (r_{u,i}\\text{が評価値行列に含まれている}) \\\\ 0 & (r_{u,i}\\text{が評価値行列で欠損している}) \\end{cases} \\quad (2\\text{-}15)$$\n",
    "\n",
    "## 指示関数を組み込んだコサイン類似度\n",
    "\n",
    "$$\\cos(u_1, u_2) = \\frac{\\sum_{i=1}^{4} \\delta_{1,i}(r_{1,i} - \\bar{u}_1) \\delta_{2,i}(r_{2,i} - \\bar{u}_2)}{\\sqrt{\\sum_{i=1}^{4} \\delta_{1,i}(r_{1,i} - \\bar{u}_1)^2} \\sqrt{\\sum_{i=1}^{4} \\delta_{2,i}(r_{2,i} - \\bar{u}_2)^2}}$$\n",
    "\n",
    "これにより、欠損している部分は計算から除外され、欠損していない要素のみに対して計算が実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価値行列R（図2.24）\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],  # user1: item3が欠損\n",
    "    [2, 5, 0, 5],  # user2: item3が欠損\n",
    "    [0, 3, 4, 4],  # user3: item1が欠損\n",
    "    [4, 2, 3, 0]   # user4: item4が欠損\n",
    "])\n",
    "\n",
    "print(\"=== 評価値行列 R ===\")\n",
    "print(R)\n",
    "print(\"\\n（0は欠損値を表す）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user1とuser2のコサイン類似度を計算（中心化・指示関数付き）\n",
    "print(\"=== cos(u1, u2) の計算 ===\")\n",
    "print()\n",
    "\n",
    "u1 = R[0]  # [2, 3, 0, 5]\n",
    "u2 = R[1]  # [2, 5, 0, 5]\n",
    "\n",
    "print(f\"user1: {u1}\")\n",
    "print(f\"user2: {u2}\")\n",
    "print()\n",
    "\n",
    "# 条件1: 評価値を付けていないitemは対象外として平均値を計算\n",
    "# user1とuser2共通で評価しているのはitem1, item2, item4\n",
    "common_items = (u1 != 0) & (u2 != 0)\n",
    "print(f\"共通して評価しているアイテム: {['item1', 'item2', 'item3', 'item4'][i] for i, v in enumerate(common_items) if v}\")\n",
    "print(f\"→ item1, item2, item4 (item3は両者とも欠損)\")\n",
    "print()\n",
    "\n",
    "# 平均値の計算（共通アイテムのみ）\n",
    "mean_u1 = np.mean(u1[common_items])\n",
    "mean_u2 = np.mean(u2[common_items])\n",
    "print(f\"user1の平均評価値: (2 + 3 + 5) / 3 = {mean_u1:.3f}\")\n",
    "print(f\"user2の平均評価値: (2 + 5 + 5) / 3 = {mean_u2:.3f}\")\n",
    "print()\n",
    "\n",
    "# 中心化\n",
    "u1_centered = u1[common_items] - mean_u1\n",
    "u2_centered = u2[common_items] - mean_u2\n",
    "print(f\"user1 (中心化後): {u1_centered}\")\n",
    "print(f\"user2 (中心化後): {u2_centered}\")\n",
    "print()\n",
    "\n",
    "# コサイン類似度の計算\n",
    "cos_u1_u2 = cosine_similarity_centered(u1, u2)\n",
    "print(f\"cos(u1, u2) = {cos_u1_u2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全ユーザー間のコサイン類似度行列を計算（図2.26）\n",
    "print(\"=== 図2.26: ユーザー間の類似度の計算結果を表形式で整理 ===\")\n",
    "print()\n",
    "\n",
    "n_users = R.shape[0]\n",
    "similarity_matrix = np.zeros((n_users, n_users))\n",
    "\n",
    "for i in range(n_users):\n",
    "    for j in range(n_users):\n",
    "        similarity_matrix[i, j] = cosine_similarity_centered(R[i], R[j])\n",
    "\n",
    "# 表形式で表示\n",
    "print(\"          user1    user2    user3    user4\")\n",
    "for i, row in enumerate(similarity_matrix):\n",
    "    print(f\"user{i+1}  \", end=\"\")\n",
    "    for val in row:\n",
    "        print(f\"{val:8.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 類似度行列のヒートマップ\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "im = ax.imshow(similarity_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "ax.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{similarity_matrix[i, j]:.3f}', ha='center', va='center', fontsize=11)\n",
    "\n",
    "ax.set_title('図2.26: ユーザー間のコサイン類似度行列')\n",
    "plt.colorbar(im, label='コサイン類似度')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"対角線は自分自身との類似度なので1.000\")\n",
    "print(\"user3とuser4の類似度が最も高い（0.894）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-9 欠損値を推計する数理モデルを設計し、計算を実行する\n",
    "\n",
    "ここまでの計算結果を用いて、これから欠損値の推計を行っていきます。\n",
    "\n",
    "## 推計対象\n",
    "\n",
    "推計の対象となる欠損値を $r_{3,1}$（user3のitem1への評価）とします。\n",
    "\n",
    "図2.26の表の計算結果を踏まえ、user3に似ている順番に他のユーザーを並べると\n",
    "$$\\cos(u_3, u_4) > \\cos(u_3, u_1) > \\cos(u_3, u_2)$$\n",
    "となるので、user3に似ているのはuser4、user1ということにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図2.27: 欠損値推計の概念図\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 評価値行列を描画\n",
    "ax.imshow(np.where(R == 0, 0.3, 0.8), cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if R[i, j] == 0:\n",
    "            if i == 2 and j == 0:  # user3のitem1（推計対象）\n",
    "                ax.text(j, i, '?', ha='center', va='center', fontsize=16, color='red', fontweight='bold')\n",
    "            else:\n",
    "                ax.text(j, i, '−', ha='center', va='center', fontsize=14, color='gray')\n",
    "        else:\n",
    "            ax.text(j, i, str(R[i, j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\n",
    "ax.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n",
    "\n",
    "# 矢印で類似ユーザーからの参照を示す\n",
    "ax.annotate('', xy=(0, 2), xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "ax.annotate('', xy=(0, 2), xytext=(0, 3),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax.text(4.5, 0, 'user1\\ncos=0.614', fontsize=10, va='center')\n",
    "ax.text(4.5, 3, 'user4\\ncos=0.894', fontsize=10, va='center')\n",
    "ax.text(4.5, 2, '← 類似ユーザーの\\n   評価値を利用', fontsize=10, va='center', color='green')\n",
    "\n",
    "ax.set_title('図2.27: user3のitem1の欠損値を、類似ユーザー（user1, user4）の評価から推計', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測値の計算式\n",
    "\n",
    "user3のitem1に対する評価値 $r_{3,1}$ を計算するには、例えば次の数式に示すような方法があります。ちなみに評価値 $r_{3,1}$ は予測値として算出されるため、予測値を示す記号^（ハット）を用いて $\\hat{r}_{3,1}$ と表記します。\n",
    "\n",
    "$$\\hat{r}_{3,1} = \\bar{u}_3 + (r_{1,1} - \\bar{u}_1) \\times \\cos(u_3, u_1) + (r_{4,1} - \\bar{u}_4) \\times \\cos(u_3, u_4)$$\n",
    "\n",
    "この数式は以下の構造になっています：\n",
    "\n",
    "- **$\\bar{u}_3$**：user3の平均評価値（ベース）\n",
    "- **$(r_{1,1} - \\bar{u}_1)$**：user1のitem1評価と平均との差\n",
    "- **$\\cos(u_3, u_1)$**：user3との類似度合い（重み付け）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図2.28: 欠損値の推計で用いる数値を表形式で整理\n",
    "print(\"=== 図2.28: 欠損値の推計で用いる数値 ===\")\n",
    "print()\n",
    "\n",
    "# user1, user3, user4の情報\n",
    "users_info = {\n",
    "    'user1': {'ratings': R[0], 'item1': R[0, 0], 'mean': None, 'cos_with_u3': similarity_matrix[2, 0]},\n",
    "    'user3': {'ratings': R[2], 'item1': '?', 'mean': None, 'cos_with_u3': '-'},\n",
    "    'user4': {'ratings': R[3], 'item1': R[3, 0], 'mean': None, 'cos_with_u3': similarity_matrix[2, 3]}\n",
    "}\n",
    "\n",
    "# 平均値を計算（共通アイテムでの平均ではなく、各ユーザーの評価平均）\n",
    "for user, info in users_info.items():\n",
    "    ratings = info['ratings']\n",
    "    valid_ratings = ratings[ratings != 0]\n",
    "    info['mean'] = np.mean(valid_ratings)\n",
    "\n",
    "print(f\"{'':10} {'item1':>8} {'平均評価値':>12} {'user3とのcos':>14}\")\n",
    "print(\"-\" * 50)\n",
    "for user, info in users_info.items():\n",
    "    item1_val = info['item1'] if isinstance(info['item1'], str) else f\"{info['item1']}\"\n",
    "    cos_val = info['cos_with_u3'] if isinstance(info['cos_with_u3'], str) else f\"{info['cos_with_u3']:.3f}\"\n",
    "    print(f\"{user:10} {item1_val:>8} {info['mean']:>12.3f} {cos_val:>14}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値を計算\n",
    "print(\"=== r̂_{3,1} の計算 ===\")\n",
    "print()\n",
    "\n",
    "# 各値を取得\n",
    "mean_u1 = users_info['user1']['mean']\n",
    "mean_u3 = users_info['user3']['mean']\n",
    "mean_u4 = users_info['user4']['mean']\n",
    "\n",
    "r_1_1 = R[0, 0]  # user1のitem1評価 = 2\n",
    "r_4_1 = R[3, 0]  # user4のitem1評価 = 4\n",
    "\n",
    "cos_u3_u1 = similarity_matrix[2, 0]  # 0.614\n",
    "cos_u3_u4 = similarity_matrix[2, 3]  # 0.894\n",
    "\n",
    "print(f\"各パラメータ:\")\n",
    "print(f\"  ū₃ = {mean_u3:.3f}\")\n",
    "print(f\"  r₁,₁ = {r_1_1}, ū₁ = {mean_u1:.3f}, (r₁,₁ - ū₁) = {r_1_1 - mean_u1:.3f}\")\n",
    "print(f\"  r₄,₁ = {r_4_1}, ū₄ = {mean_u4:.3f}, (r₄,₁ - ū₄) = {r_4_1 - mean_u4:.3f}\")\n",
    "print(f\"  cos(u₃, u₁) = {cos_u3_u1:.3f}\")\n",
    "print(f\"  cos(u₃, u₄) = {cos_u3_u4:.3f}\")\n",
    "print()\n",
    "\n",
    "# 予測値を計算\n",
    "r_hat_3_1 = mean_u3 + (r_1_1 - mean_u1) * cos_u3_u1 + (r_4_1 - mean_u4) * cos_u3_u4\n",
    "\n",
    "print(f\"計算式:\")\n",
    "print(f\"r̂₃,₁ = ū₃ + (r₁,₁ - ū₁) × cos(u₃, u₁) + (r₄,₁ - ū₄) × cos(u₃, u₄)\")\n",
    "print()\n",
    "print(f\"     = {mean_u3:.3f} + ({r_1_1 - mean_u1:.3f}) × {cos_u3_u1:.3f} + ({r_4_1 - mean_u4:.3f}) × {cos_u3_u4:.3f}\")\n",
    "print(f\"     = {mean_u3:.3f} + {(r_1_1 - mean_u1) * cos_u3_u1:.3f} + {(r_4_1 - mean_u4) * cos_u3_u4:.3f}\")\n",
    "print(f\"     = {r_hat_3_1:.3f}\")\n",
    "print()\n",
    "print(f\"予測結果: user3のitem1への評価値は {r_hat_3_1:.3f} と推計されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の解釈\n",
    "print(\"=== 予測結果の解釈 ===\")\n",
    "print()\n",
    "print(f\"計算結果は {r_hat_3_1:.3f} と出ました。\")\n",
    "print()\n",
    "print(\"user3はitem2に3、item3に4、item4に4の評価値を付与しているので、\")\n",
    "print(f\"user3にとって {r_hat_3_1:.3f} という値は平均的な値とも考えられそうです。\")\n",
    "print()\n",
    "print(\"であれば、そこまで積極的にitem1をレコメンドする必要はなさそうですし、\")\n",
    "print(\"レコメンドしたとしても購入される確率はそこまで高くなさそうだ、と推察されます。\")\n",
    "print()\n",
    "print(\"ただし、この計算方法はあくまで一例であり、必ずしもこの通りに評価値を推定する\")\n",
    "print(\"必要はありません。どのように設計すれば目的にかなう数理モデルとなるか、常に\")\n",
    "print(\"考察と検証を繰り返すことが極めて重要です。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# まとめ\n",
    "\n",
    "## 本章で学んだこと\n",
    "\n",
    "### 評価値行列\n",
    "- ユーザーの評価をベクトルとして表現\n",
    "- 複数ユーザーの評価を行列 $R$ としてまとめる\n",
    "- 欠損値（0）= レコメンド対象\n",
    "\n",
    "### 協調フィルタリング\n",
    "- 類似ユーザーの評価を使って欠損値を予測\n",
    "- 「類似性」をコサイン類似度で定量化\n",
    "\n",
    "### コサイン類似度\n",
    "$$\\cos(a, b) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} = \\frac{\\sum_{k=1}^{n} a_k b_k}{\\sqrt{\\sum_{k=1}^{n} a_k^2} \\sqrt{\\sum_{k=1}^{n} b_k^2}}$$\n",
    "\n",
    "### 中心化（改良版）\n",
    "$$\\cos(a, b) = \\frac{\\sum_{k=1}^{n} (a_k - \\bar{a})(b_k - \\bar{b})}{\\sqrt{\\sum_{k=1}^{n} (a_k - \\bar{a})^2} \\sqrt{\\sum_{k=1}^{n} (b_k - \\bar{b})^2}}$$\n",
    "\n",
    "### 指示関数\n",
    "$$\\delta_{u,i} = \\begin{cases} 1 & (r_{u,i}\\text{が評価値行列に含まれている}) \\\\ 0 & (r_{u,i}\\text{が評価値行列で欠損している}) \\end{cases}$$\n",
    "\n",
    "### 欠損値の予測\n",
    "$$\\hat{r}_{3,1} = \\bar{u}_3 + \\sum_{u \\in \\text{similar users}} (r_{u,1} - \\bar{u}) \\times \\cos(u_3, u)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終確認：学んだ関数のまとめ\n",
    "print(\"=== 本Notebookで実装した関数 ===\")\n",
    "print()\n",
    "print(\"1. cosine_similarity(a, b)\")\n",
    "print(\"   - 基本的なコサイン類似度を計算\")\n",
    "print()\n",
    "print(\"2. cosine_similarity_centered(a, b)\")\n",
    "print(\"   - 中心化を行ったコサイン類似度を計算\")\n",
    "print(\"   - 欠損値（0）を自動的に除外\")\n",
    "print()\n",
    "print(\"=== 使用例 ===\")\n",
    "print(f\"基本版: cosine_similarity([2,3,5], [2,5,5]) = {cosine_similarity([2,3,5], [2,5,5]):.4f}\")\n",
    "print(f\"中心化版: cosine_similarity_centered([2,3,0,5], [2,5,0,5]) = {cosine_similarity_centered([2,3,0,5], [2,5,0,5]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-10 アイテム同士の類似度で予測値を推計する\n\n前節まではuserを軸とした予測値の算出について解説しました。一方、類似度の計算方法としてはitem同士の類似度を用いることも可能です。\n\n## userベースとitemベースの比較\n\n| 手法 | 説明 |\n|:---|:---|\n| ①userを軸とした予測値の算出 | 似ているユーザーの評価値を利用 |\n| ②itemを軸とした予測値の算出 | 似ているアイテムの評価値を利用 |\n\n実際にECサイトで協調フィルタリングによって欠損値を推定する際は、**user軸よりもitem軸で予測値を算出することが実務上多い**ように見受けられます。\n\n- ECサイトで扱うitem数と訪問してくるuser数を比較すると、user数のほうが多いことが通常\n- user同士の類似度よりitem同士の類似度のほうが計算負荷が低くなりやすい",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.30: アイテム間の類似度行列\nprint(\"=== 図2.30: 各item同士の類似度は表形式で整理できる ===\")\nprint()\nprint(\"          item1    item2    item3    item4\")\nprint(\"item1   cos(i₁,i₁) cos(i₁,i₂) cos(i₁,i₃) cos(i₁,i₄)\")\nprint(\"item2   cos(i₂,i₁) cos(i₂,i₂) cos(i₂,i₃) cos(i₂,i₄)\")\nprint(\"item3   cos(i₃,i₁) cos(i₃,i₂) cos(i₃,i₃) cos(i₃,i₄)\")\nprint(\"item4   cos(i₄,i₁) cos(i₄,i₂) cos(i₄,i₃) cos(i₄,i₄)\")\nprint()\nprint(\"アイテムベースでは、評価値行列Rの「列ベクトル」を使って類似度を計算します。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 評価値行列R（再掲）\nR = np.array([\n    [2, 3, 0, 5],\n    [2, 5, 0, 5],\n    [0, 3, 4, 4],\n    [4, 2, 3, 0]\n])\n\nprint(\"評価値行列 R:\")\nprint(R)\nprint()\n\n# 列ベクトルとして各アイテムを表現\ni1 = R[:, 0]  # item1の列ベクトル\ni2 = R[:, 1]  # item2の列ベクトル\ni3 = R[:, 2]  # item3の列ベクトル\ni4 = R[:, 3]  # item4の列ベクトル\n\nprint(\"各アイテムの列ベクトル:\")\nprint(f\"i₁ = {i1}\")\nprint(f\"i₂ = {i2}\")\nprint(f\"i₃ = {i3}\")\nprint(f\"i₄ = {i4}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## アイテムベースのコサイン類似度関数\n\nアイテムベースでも同様に、指示関数を用いて欠損値を除外し、中心化を行います。\n\n$$\\cos(i_1, i_4) = \\frac{\\sum_{u=1}^{4} \\delta_{u,1}(r_{u,1} - \\bar{u}_u) \\delta_{u,4}(r_{u,4} - \\bar{u}_u)}{\\sqrt{\\sum_{u=1}^{4} \\delta_{u,1}(r_{u,1} - \\bar{u}_u)^2} \\sqrt{\\sum_{u=1}^{4} \\delta_{u,4}(r_{u,4} - \\bar{u}_u)^2}}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# アイテムベースのコサイン類似度関数\ndef cosine_similarity_item_based(R, item_i, item_j):\n    \"\"\"\n    アイテムベースの中心化コサイン類似度を計算する\n    \n    Parameters:\n    -----------\n    R : numpy.ndarray\n        評価値行列\n    item_i, item_j : int\n        比較するアイテムのインデックス\n    \n    Returns:\n    --------\n    float\n        アイテム間のコサイン類似度\n    \"\"\"\n    col_i = R[:, item_i]\n    col_j = R[:, item_j]\n    \n    # 両方のアイテムを評価しているユーザーのみ\n    common_users = (col_i != 0) & (col_j != 0)\n    \n    if np.sum(common_users) == 0:\n        return 0.0\n    \n    # 各ユーザーの平均評価値で中心化\n    numerator = 0.0\n    norm_i_sq = 0.0\n    norm_j_sq = 0.0\n    \n    for u in range(R.shape[0]):\n        if common_users[u]:\n            # ユーザーuの平均評価値（欠損値除く）\n            user_ratings = R[u, :]\n            valid_ratings = user_ratings[user_ratings != 0]\n            mean_u = np.mean(valid_ratings)\n            \n            centered_i = col_i[u] - mean_u\n            centered_j = col_j[u] - mean_u\n            \n            numerator += centered_i * centered_j\n            norm_i_sq += centered_i ** 2\n            norm_j_sq += centered_j ** 2\n    \n    if norm_i_sq == 0 or norm_j_sq == 0:\n        return 0.0\n    \n    return numerator / (np.sqrt(norm_i_sq) * np.sqrt(norm_j_sq))\n\nprint(\"cosine_similarity_item_based関数を定義しました\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# cos(i1, i4)を計算\nprint(\"=== cos(i₁, i₄) の計算 ===\")\nprint()\n\n# item1とitem4を共通で評価しているのはuser1とuser2\nprint(\"item1とitem4を共通で評価しているユーザー: user1, user2\")\nprint(\"(user3はitem1が欠損、user4はitem4が欠損)\")\nprint()\n\n# user1とuser2の平均評価値\nmean_u1 = np.mean([2, 3, 5])  # user1: [2,3,0,5] → 欠損除外\nmean_u2 = np.mean([2, 5, 5])  # user2: [2,5,0,5] → 欠損除外\nprint(f\"ū₁ = (2+3+5)/3 = {mean_u1:.3f}\")\nprint(f\"ū₂ = (2+5+5)/3 = {mean_u2:.3f}\")\nprint()\n\n# 中心化した値\nprint(\"中心化:\")\nprint(f\"  user1: (r₁,₁ - ū₁) = 2 - {mean_u1:.3f} = {2 - mean_u1:.3f}\")\nprint(f\"         (r₁,₄ - ū₁) = 5 - {mean_u1:.3f} = {5 - mean_u1:.3f}\")\nprint(f\"  user2: (r₂,₁ - ū₂) = 2 - {mean_u2:.3f} = {2 - mean_u2:.3f}\")\nprint(f\"         (r₂,₄ - ū₂) = 5 - {mean_u2:.3f} = {5 - mean_u2:.3f}\")\nprint()\n\n# コサイン類似度の計算\ncos_i1_i4 = cosine_similarity_item_based(R, 0, 3)\nprint(f\"cos(i₁, i₄) = {cos_i1_i4:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 全アイテム間のコサイン類似度行列を計算\nprint(\"=== アイテム間の類似度行列 ===\")\nprint()\n\nn_items = R.shape[1]\nitem_similarity_matrix = np.zeros((n_items, n_items))\n\nfor i in range(n_items):\n    for j in range(n_items):\n        item_similarity_matrix[i, j] = cosine_similarity_item_based(R, i, j)\n\n# 表形式で表示\nprint(\"          item1    item2    item3    item4\")\nfor i, row in enumerate(item_similarity_matrix):\n    print(f\"item{i+1}  \", end=\"\")\n    for val in row:\n        print(f\"{val:8.3f} \", end=\"\")\n    print()\n\nprint()\nprint(f\"cos(i₁, i₄) = {item_similarity_matrix[0, 3]:.3f}\")\nprint(f\"cos(i₂, i₄) = {item_similarity_matrix[1, 3]:.3f}\")\nprint(f\"cos(i₃, i₄) = {item_similarity_matrix[2, 3]:.3f}\")\nprint()\nprint(\"→ item4に最も似ているのはitem3（cos=1.000）、次にitem2（cos=0.090）\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## アイテムベースでの欠損値予測\n\nuser4のitem4（$r_{4,4}$）の欠損値を、item軸で推計してみましょう。\n\nitem4に似ているのはitem2とitem3なので、user4のitem2, item3に対する評価値を参考にします。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.31: 類似度の高いアイテムの評価値を用いて欠損値を推計\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.imshow(np.where(R == 0, 0.3, 0.8), cmap='Blues', vmin=0, vmax=1)\n\nfor i in range(4):\n    for j in range(4):\n        if R[i, j] == 0:\n            if i == 3 and j == 3:  # user4のitem4（推計対象）\n                ax.text(j, i, '?', ha='center', va='center', fontsize=16, color='red', fontweight='bold')\n            else:\n                ax.text(j, i, '−', ha='center', va='center', fontsize=14, color='gray')\n        else:\n            ax.text(j, i, str(R[i, j]), ha='center', va='center', fontsize=14)\n\nax.set_xticks(range(4))\nax.set_yticks(range(4))\nax.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\nax.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\n\n# 矢印で類似アイテムからの参照を示す\nax.annotate('', xy=(3, 3), xytext=(1, 3),\n            arrowprops=dict(arrowstyle='->', color='green', lw=2))\nax.annotate('', xy=(3, 3), xytext=(2, 3),\n            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n\nax.set_title('図2.31: user4のitem4の欠損値を、類似アイテム（item2, item3）の評価から推計', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"user4の評価値: item1=4, item2=2, item3=3, item4=?\")\nprint(f\"cos(i₂, i₄) = {item_similarity_matrix[1, 3]:.3f}\")\nprint(f\"cos(i₃, i₄) = {item_similarity_matrix[2, 3]:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-11 ユーザー目線で数理モデルを再考する ーセレンディピティー\n\nここまでuser軸とitem軸の2つの観点から、評価値を予測するための数理モデルを解説してきました。ここで、今一度レコメンドの数理モデルについて考え直してみましょう。\n\n## セレンディピティとは\n\nECサイトを考える上で重要な観点の1つに、**セレンディピティ**という概念があります。これは「目新しさ」といった意味合いで使われる概念です。\n\n### item軸の問題点\n\nitem軸でのレコメンドを運用していると起きがちな現象：\n- ECサイトに訪問するたびに毎回同じような商品ばかりがレコメンドされる\n- ユーザーは見飽きてしまい、そのECサイトを利用しなくなることが懸念される\n- 似ているアイテムばかりがレコメンドされても、そのユーザーが既に類似商品を持っていたりすると、レコメンドした商品への興味は薄い可能性が高い",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# セレンディピティの概念図\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左：item軸の問題点\nax1 = axes[0]\nax1.text(0.5, 0.9, 'item軸でのレコメンド', ha='center', fontsize=12, fontweight='bold', transform=ax1.transAxes)\nax1.text(0.5, 0.7, '毎回似たような商品ばかり', ha='center', fontsize=10, color='red', transform=ax1.transAxes)\n\n# 同じような商品のイメージ\nfor i in range(3):\n    ax1.add_patch(plt.Rectangle((0.2 + i*0.2, 0.3), 0.15, 0.2, fill=True, color='lightblue', alpha=0.8))\n    ax1.text(0.275 + i*0.2, 0.4, f'商品{i+1}', ha='center', fontsize=9)\n\nax1.text(0.5, 0.15, '→ ユーザーは「また同じか...」と飽きてしまう', ha='center', fontsize=10, transform=ax1.transAxes)\nax1.axis('off')\nax1.set_title('問題：セレンディピティの欠如')\n\n# 右：user軸の利点\nax2 = axes[1]\nax2.text(0.5, 0.9, 'user軸でのレコメンド', ha='center', fontsize=12, fontweight='bold', transform=ax2.transAxes)\nax2.text(0.5, 0.7, '自分と嗜好が似ているユーザーが\\n興味を持っている商品を発見', ha='center', fontsize=10, color='green', transform=ax2.transAxes)\n\n# 異なる商品のイメージ\ncolors = ['lightblue', 'lightgreen', 'lightyellow']\nfor i, color in enumerate(colors):\n    ax2.add_patch(plt.Rectangle((0.2 + i*0.2, 0.3), 0.15, 0.2, fill=True, color=color, alpha=0.8))\n    ax2.text(0.275 + i*0.2, 0.4, f'新発見{i+1}', ha='center', fontsize=9)\n\nax2.text(0.5, 0.15, '→ UX（User Experience）の向上が期待できる', ha='center', fontsize=10, transform=ax2.transAxes)\nax2.axis('off')\nax2.set_title('解決：意外な商品との出会い')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"セレンディピティ = 思いがけない発見、偶然の出会い\")\nprint()\nprint(\"user軸でのレコメンドは、自分と嗜好が似ている他のユーザーが興味を持っている\")\nprint(\"アイテムに基づき欠損値を予測するため、item軸ではレコメンドされなかった\")\nprint(\"商品に出会える可能性がある。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 協調フィルタリングの限界\n\nつまり、item軸とuser軸、どちらのアプローチで欠損値を予測したとしても、セレンディピティという観点では優れたレコメンドを実現できない可能性がある、ということです。\n\nでは、セレンディピティという観点から考えると、どのような数理モデルによって評価値を予測すればよいのでしょうか。\n\nこの課題を解決するために、次節からより高度な数理モデルを考察します。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-12 課題解決のために数理モデルを変更する ー行列因子分解ー\n\n本節以降では、「③行列因子分解（MF: Matrix Factorization）」によって評価値を推定する数理モデルを導出します。その準備として、まずは**残差行列**を導出し、行列の**和**、**差**及び**積**への理解が必要となります。\n\n## 基礎となる数理的手法\n\n### 行列の和\n$$A + B = \\begin{pmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{pmatrix} + \\begin{pmatrix} b_{1,1} & b_{1,2} \\\\ b_{2,1} & b_{2,2} \\end{pmatrix} = \\begin{pmatrix} a_{1,1}+b_{1,1} & a_{1,2}+b_{1,2} \\\\ a_{2,1}+b_{2,1} & a_{2,2}+b_{2,2} \\end{pmatrix}$$\n\n### 行列の差\n$$A - B = \\begin{pmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{pmatrix} - \\begin{pmatrix} b_{1,1} & b_{1,2} \\\\ b_{2,1} & b_{2,2} \\end{pmatrix} = \\begin{pmatrix} a_{1,1}-b_{1,1} & a_{1,2}-b_{1,2} \\\\ a_{2,1}-b_{2,1} & a_{2,2}-b_{2,2} \\end{pmatrix}$$\n\n### 行列の積\n$$AB = \\begin{pmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{pmatrix} \\begin{pmatrix} b_{1,1} & b_{1,2} \\\\ b_{2,1} & b_{2,2} \\end{pmatrix} = \\begin{pmatrix} a_{1,1}b_{1,1}+a_{1,2}b_{2,1} & a_{1,1}b_{1,2}+a_{1,2}b_{2,2} \\\\ a_{2,1}b_{1,1}+a_{2,2}b_{2,1} & a_{2,1}b_{1,2}+a_{2,2}b_{2,2} \\end{pmatrix}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 行列演算の基礎\nprint(\"=== 行列演算の例 ===\")\nprint()\n\nA = np.array([[1, 2], [3, 1]])\nB = np.array([[2, 3], [1, 2]])\n\nprint(\"行列A:\")\nprint(A)\nprint()\nprint(\"行列B:\")\nprint(B)\nprint()\n\nprint(\"--- 行列の和 A + B ---\")\nprint(A + B)\nprint()\n\nprint(\"--- 行列の差 A - B ---\")\nprint(A - B)\nprint()\n\nprint(\"--- 行列の積 AB ---\")\nprint(A @ B)\nprint()\nprint(\"計算過程:\")\nprint(f\"  (1,1)要素: 1×2 + 2×1 = {1*2 + 2*1}\")\nprint(f\"  (1,2)要素: 1×3 + 2×2 = {1*3 + 2*2}\")\nprint(f\"  (2,1)要素: 3×2 + 1×1 = {3*2 + 1*1}\")\nprint(f\"  (2,2)要素: 3×3 + 1×2 = {3*3 + 1*2}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 行列積の規則性\n\n行列同士の積の計算結果には規則性があります。重要なのは行列の**形**が重要だということです。\n\n**規則**: $m \\times n$ 行列と $n \\times p$ 行列の積は $m \\times p$ 行列になる\n\n$$\\underbrace{(2 \\times 3)}_{A} \\times \\underbrace{(3 \\times 2)}_{B} = \\underbrace{(2 \\times 2)}_{AB}$$\n\n隣り合う数字が同じ（上の例では3）でないと、行列の積は計算できません。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.36: 行列積が可能な場合と不可の場合\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# 左：計算可能\nax1 = axes[0]\nax1.text(0.1, 0.6, '4×2', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightblue'))\nax1.text(0.25, 0.6, '×', fontsize=14, ha='center')\nax1.text(0.4, 0.6, '2×3', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen'))\nax1.text(0.55, 0.6, '=', fontsize=14, ha='center')\nax1.text(0.7, 0.6, '4×3', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n\n# 矢印で一致を示す\nax1.annotate('', xy=(0.35, 0.45), xytext=(0.15, 0.45),\n            arrowprops=dict(arrowstyle='<->', color='green', lw=2))\nax1.text(0.25, 0.35, '一致!', fontsize=10, ha='center', color='green')\n\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.axis('off')\nax1.set_title('計算可能', fontsize=12)\n\n# 右：計算不可\nax2 = axes[1]\nax2.text(0.1, 0.6, '4×2', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightblue'))\nax2.text(0.25, 0.6, '×', fontsize=14, ha='center')\nax2.text(0.4, 0.6, '3×2', fontsize=14, ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral'))\nax2.text(0.55, 0.6, '=', fontsize=14, ha='center')\nax2.text(0.7, 0.6, '×', fontsize=20, ha='center', color='red')\n\n# 矢印で不一致を示す\nax2.annotate('', xy=(0.35, 0.45), xytext=(0.15, 0.45),\n            arrowprops=dict(arrowstyle='<->', color='red', lw=2))\nax2.text(0.25, 0.35, '不一致!', fontsize=10, ha='center', color='red')\n\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.axis('off')\nax2.set_title('計算できない', fontsize=12)\n\nplt.suptitle('図2.36: 行列積が可能な場合と不可の場合', fontsize=12)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 残差行列\n\n評価値行列 $R$ と予測値行列 $\\hat{R}$ の差を**残差行列** $E$ と呼びます。\n\n$$E = R - \\hat{R} = R - PQ$$\n\nここで：\n- $R$: 評価値行列（$U \\times I$ 行列）\n- $\\hat{R}$: 予測値行列（$U \\times I$ 行列）\n- $P$: ユーザー因子行列（$U \\times d$ 行列）\n- $Q$: アイテム因子行列（$d \\times I$ 行列）\n- $d$: **潜在因子**の数",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 潜在因子モデル\n\n評価値行列がスパース（疎）な行列となりがちです。このような評価値行列が抱える実務上の問題に対応するために、数理モデルとして**潜在因子モデル**がしばしば活用されます。\n\n潜在因子モデルでは、ユーザーとアイテムの関係を、いくつかの**潜在因子**に基づいて表現します。潜在因子とは、例えばユーザーの隠れた嗜好やアイテムの隠れた特性を反映しており、具体的な意味内容を持つこともあれば、データから抽出される抽象的な（意味付けることが難しい）特徴であることもあります。\n\n## 行列因子分解（Matrix Factorization）\n\n行列因子分解のアプローチでは、評価値行列 $R$ を**ユーザー因子行列** $P$ と**アイテム因子行列** $Q$ の積に分解します。\n\n$$R \\approx P \\times Q$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.34: 行列因子分解の概念図\nfig, ax = plt.subplots(figsize=(14, 5))\n\n# R (U×I)\nax.add_patch(plt.Rectangle((0.05, 0.3), 0.15, 0.4, fill=True, color='lightblue', alpha=0.8))\nax.text(0.125, 0.5, 'R', fontsize=20, ha='center', va='center', fontweight='bold')\nax.text(0.125, 0.75, 'U×I', fontsize=10, ha='center')\nax.text(0.125, 0.2, '評価値行列', fontsize=9, ha='center')\n\n# ≈\nax.text(0.25, 0.5, '≈', fontsize=24, ha='center', va='center')\n\n# P (U×d)\nax.add_patch(plt.Rectangle((0.32, 0.3), 0.08, 0.4, fill=True, color='lightgreen', alpha=0.8))\nax.text(0.36, 0.5, 'P', fontsize=20, ha='center', va='center', fontweight='bold')\nax.text(0.36, 0.75, 'U×d', fontsize=10, ha='center')\nax.text(0.36, 0.2, 'ユーザー\\n因子行列', fontsize=9, ha='center')\n\n# ×\nax.text(0.45, 0.5, '×', fontsize=20, ha='center', va='center')\n\n# Q (d×I)\nax.add_patch(plt.Rectangle((0.52, 0.4), 0.15, 0.15, fill=True, color='lightyellow', alpha=0.8))\nax.text(0.595, 0.475, 'Q', fontsize=20, ha='center', va='center', fontweight='bold')\nax.text(0.595, 0.6, 'd×I', fontsize=10, ha='center')\nax.text(0.595, 0.3, 'アイテム因子行列', fontsize=9, ha='center')\n\n# 説明\nax.text(0.8, 0.6, 'd = 潜在因子の数', fontsize=11, ha='left')\nax.text(0.8, 0.5, 'U = ユーザー数', fontsize=11, ha='left')\nax.text(0.8, 0.4, 'I = アイテム数', fontsize=11, ha='left')\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('図2.34: 評価値行列Rをユーザー因子行列Pとアイテム因子行列Qに分解する', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"行列PとQは元の評価値行列Rよりも低ランクの行列となる。\")\nprint(\"dは潜在因子の数で、ハイパーパラメータとして設定する。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-13 評価値の推計を最適化問題に置き換える ー残差行列と誤差ー\n\nでは、具体的な数理モデルの考察に入ります。このケースでもuser軸、item軸で考察した評価値行列を用います。\n\n## 評価値行列の一般化表現\n\n$$R = \\begin{pmatrix} r_{1,1} & r_{1,2} & r_{1,3} & r_{1,4} \\\\ r_{2,1} & r_{2,2} & r_{2,3} & r_{2,4} \\\\ r_{3,1} & r_{3,2} & r_{3,3} & r_{3,4} \\\\ r_{4,1} & r_{4,2} & r_{4,3} & r_{4,4} \\end{pmatrix} = \\begin{pmatrix} 2 & 3 & 0 & 5 \\\\ 2 & 5 & 0 & 5 \\\\ 0 & 3 & 4 & 4 \\\\ 4 & 2 & 3 & 0 \\end{pmatrix}$$\n\n## 潜在因子行列の表現\n\n$P$ は $4 \\times 2$ 行列、$Q$ は $2 \\times 4$ 行列です。\n\n$$P = \\begin{pmatrix} p_{1,1} & p_{1,2} \\\\ p_{2,1} & p_{2,2} \\\\ p_{3,1} & p_{3,2} \\\\ p_{4,1} & p_{4,2} \\end{pmatrix}, \\quad Q = \\begin{pmatrix} q_{1,1} & q_{1,2} & q_{1,3} & q_{1,4} \\\\ q_{2,1} & q_{2,2} & q_{2,3} & q_{2,4} \\end{pmatrix}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# P, Qの一般化表現\nprint(\"=== 潜在因子行列の設定 ===\")\nprint()\n\n# d=2として設定\nd = 2\nn_users = 4\nn_items = 4\n\nprint(f\"潜在因子数 d = {d}\")\nprint(f\"ユーザー数 U = {n_users}\")\nprint(f\"アイテム数 I = {n_items}\")\nprint()\n\nprint(f\"ユーザー因子行列 P: {n_users}×{d} 行列\")\nprint(f\"アイテム因子行列 Q: {d}×{n_items} 行列\")\nprint()\n\n# 行列Pの要素\nprint(\"P の要素:\")\nfor u in range(1, n_users+1):\n    row = [f\"p_{u},{k}\" for k in range(1, d+1)]\n    print(f\"  user{u}: [{', '.join(row)}]\")\n\nprint()\n\n# 行列Qの要素\nprint(\"Q の要素:\")\nfor k in range(1, d+1):\n    row = [f\"q_{k},{i}\" for i in range(1, n_items+1)]\n    print(f\"  潜在因子{k}: [{', '.join(row)}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 予測値の計算\n\n$PQ$ の積を計算すると、各要素は以下のようになります。\n\n$$\\hat{r}_{u,i} = \\sum_{k=1}^{d} p_{u,k} \\cdot q_{k,i}$$\n\n例えば $\\hat{r}_{1,3}$（user1のitem3の予測値）は：\n\n$$\\hat{r}_{1,3} = p_{1,1} \\cdot q_{1,3} + p_{1,2} \\cdot q_{2,3}$$\n\n## 残差（誤差）\n\n評価値と予測値の差（残差）は：\n\n$$e_{u,i} = r_{u,i} - \\hat{r}_{u,i} = r_{u,i} - \\sum_{k=1}^{d} p_{u,k} \\cdot q_{k,i}$$\n\n**行列因子分解の目標は、この残差を最小化するようなPとQを見つけること**です。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 残差の計算例\nprint(\"=== 残差の計算例 ===\")\nprint()\n\n# 評価値行列\nR = np.array([\n    [2, 3, 0, 5],\n    [2, 5, 0, 5],\n    [0, 3, 4, 4],\n    [4, 2, 3, 0]\n])\n\n# ランダムな初期値でP, Qを設定（実際は最適化で求める）\nnp.random.seed(42)\nP_init = np.random.rand(4, 2)\nQ_init = np.random.rand(2, 4)\n\nprint(\"初期のユーザー因子行列 P:\")\nprint(np.round(P_init, 3))\nprint()\n\nprint(\"初期のアイテム因子行列 Q:\")\nprint(np.round(Q_init, 3))\nprint()\n\n# 予測行列 R̂ = PQ\nR_hat = P_init @ Q_init\n\nprint(\"予測行列 R̂ = PQ:\")\nprint(np.round(R_hat, 3))\nprint()\n\n# 残差行列 E = R - R̂（欠損値は除く）\nE = R - R_hat\nprint(\"残差行列 E = R - R̂:\")\nprint(np.round(E, 3))\nprint()\n\n# 欠損値を除いた二乗誤差の合計\nmask = (R != 0)\nsquared_error = np.sum((E[mask]) ** 2)\nprint(f\"二乗誤差の合計（欠損値除く）: {squared_error:.3f}\")\nprint()\nprint(\"この二乗誤差を最小化するようにP, Qを更新していきます。\")\nprint(\"これが「最適化問題」としての行列因子分解です。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# まとめ（Part 2）\n\n## 2-10〜2-13で学んだこと\n\n### アイテムベース協調フィルタリング\n- 列ベクトルでアイテムを表現\n- アイテム間のコサイン類似度を計算\n- 類似アイテムの評価値から欠損値を予測\n\n### セレンディピティ\n- レコメンドにおける「目新しさ」の重要性\n- item軸だけでは似た商品ばかりになりがち\n- ユーザー体験（UX）の観点からの課題\n\n### 行列因子分解（Matrix Factorization）\n$$R \\approx P \\times Q$$\n\n- $R$: 評価値行列（$U \\times I$）\n- $P$: ユーザー因子行列（$U \\times d$）\n- $Q$: アイテム因子行列（$d \\times I$）\n- $d$: 潜在因子の数（ハイパーパラメータ）\n\n### 残差と最適化\n$$e_{u,i} = r_{u,i} - \\sum_{k=1}^{d} p_{u,k} \\cdot q_{k,i}$$\n\n目標：残差（二乗誤差）を最小化する $P$, $Q$ を見つける\n\n## 次のステップ（教科書の続き）\n- 損失関数の定義\n- 勾配降下法による最適化\n- 正則化項の追加",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-14 最適化問題を解く ー損失関数ー\n\nこの最適化問題を解く手法にはさまざまなパターンがありますが、本章では基本的かつ根本的な考え方を学べる**勾配降下法**を採用して考えていきます。\n\nその準備として、本節では**損失関数**と呼ばれる数理的手法を紹介して解説していきます。\n\n## 基礎となる数理的手法と情報工学的アプローチ\n\n| 基礎となる数理的手法 | 情報工学的アプローチ |\n|:---|:---|\n| 総和記号：$a_1b_1 + a_2b_2 + \\cdots + a_nb_n = \\sum_{k=1}^{n} a_k b_k$ | 損失関数の設計 |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 残差行列 E の再掲\n\n先程 $R - \\hat{R} = E$ という数式を示しましたが、この値を最小化するとはつまり、計算結果となる $E$ のすべての成分の和を0に近づけるということです。\n\n$$E = R - \\hat{R} = R - PQ$$\n\n残差行列 $E$ の各要素は：\n\n$$e_{u,i} = r_{u,i} - (p_{u,1}q_{1,i} + p_{u,2}q_{2,i}) = r_{u,i} - \\sum_{d=1}^{2} p_{u,d}q_{d,i}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 残差行列の具体例\nprint(\"=== 残差行列 E の計算 ===\")\nprint()\n\n# 評価値行列\nR = np.array([\n    [2, 3, 0, 5],\n    [2, 5, 0, 5],\n    [0, 3, 4, 4],\n    [4, 2, 3, 0]\n])\n\nprint(\"評価値行列 R:\")\nprint(R)\nprint()\n\n# 仮のP, Q行列（d=2）\nP = np.array([\n    [0.5, 1.5],\n    [0.8, 1.8],\n    [1.2, 1.0],\n    [1.5, 0.3]\n])\n\nQ = np.array([\n    [1.0, 0.8, 1.5, 2.0],\n    [1.2, 1.5, 1.0, 1.8]\n])\n\nprint(\"ユーザー因子行列 P:\")\nprint(P)\nprint()\n\nprint(\"アイテム因子行列 Q:\")\nprint(Q)\nprint()\n\n# 予測行列\nR_hat = P @ Q\nprint(\"予測行列 R̂ = PQ:\")\nprint(np.round(R_hat, 2))\nprint()\n\n# 残差行列\nE = R - R_hat\nprint(\"残差行列 E = R - R̂:\")\nprint(np.round(E, 2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 損失関数 L の定義\n\nこの残差行列のすべての成分の和を0にするという点について、より考察しやすい形の数式を作ってみましょう。\n\n残差行列 $E$ のすべての成分の和を計算しているこの数式を**損失関数** $L$ と定義しました。この損失関数は、他にも**目的関数**や**誤差関数**といった呼び方をされることもあります。\n\n$$L = \\{r_{1,1} - (p_{1,1}q_{1,1} + p_{1,2}q_{2,1})\\} + \\cdots + \\{r_{4,4} - (p_{4,1}q_{1,4} + p_{4,2}q_{2,4})\\}$$\n\n### 総和記号 Σ を使った表現\n\nこの長い数式をシンプルに表すために、$\\sum$ を用います。\n\n$$L = \\sum_{u=1}^{4} \\sum_{i=1}^{4} \\{r_{u,i} - (p_{u,1}q_{1,i} + p_{u,2}q_{2,i})\\}$$\n\nさらにシンプルに：\n\n$$L = \\sum_{u=1}^{4} \\sum_{i=1}^{4} \\left\\{r_{u,i} - \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)\\right\\}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 損失関数の計算（単純な和）\nprint(\"=== 損失関数 L の計算（単純な和） ===\")\nprint()\n\n# 欠損値を除いた残差の和\nmask = (R != 0)\nL_simple = np.sum(E[mask])\n\nprint(\"残差行列 E（欠損値部分を除く）:\")\nE_masked = E.copy()\nE_masked[~mask] = np.nan\nprint(np.round(E_masked, 2))\nprint()\n\nprint(f\"損失関数 L（単純な和）= {L_simple:.3f}\")\nprint()\nprint(\"問題：残差に正と負の値が混在しているため、単純な和では打ち消し合ってしまう！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## なぜ二乗するのか\n\n損失関数の値を小さくすることが目的なので、計算結果だけを見ると0となっていて成功しているように見えます。しかし、$r_{1,1} - (p_{1,1}q_{1,1} + p_{1,2}q_{2,1})$ の計算結果は3、$r_{1,4} - (p_{1,1}q_{1,4} + p_{1,2}q_{2,4})$ の計算結果は-5で、大きく予測が外れていることがわかります。\n\nつまり、各要素の計算結果に正負の値が混ざると、最小化を考える上で不都合が生じるということです。\n\n**この問題を解消するために、2乗という操作を組み込みます。**\n\n- 負の値は2乗すると正の値、すなわちプラスになる\n- 2乗した各要素をすべて足した上でその総和が最小となるようにする\n\n$$L = \\sum_{u=1}^{4} \\sum_{i=1}^{4} \\left\\{r_{u,i} - \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)\\right\\}^2$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 二乗誤差の損失関数\nprint(\"=== 損失関数 L の計算（二乗誤差） ===\")\nprint()\n\n# 欠損値を除いた二乗誤差の和\nL_squared = np.sum((E[mask]) ** 2)\n\nprint(\"残差の二乗:\")\nE_squared = E ** 2\nE_squared_masked = E_squared.copy()\nE_squared_masked[~mask] = np.nan\nprint(np.round(E_squared_masked, 2))\nprint()\n\nprint(f\"損失関数 L（二乗誤差の和）= {L_squared:.3f}\")\nprint()\nprint(\"二乗することで:\")\nprint(\"  1. 正負の打ち消し合いが起きない\")\nprint(\"  2. 大きな誤差ほどペナルティが大きくなる\")\nprint(\"  3. 微分が容易になる（後述）\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 損失関数の実装\ndef compute_loss(R, P, Q):\n    \"\"\"\n    二乗誤差の損失関数を計算する\n    \n    Parameters:\n    -----------\n    R : numpy.ndarray\n        評価値行列（0は欠損値）\n    P : numpy.ndarray\n        ユーザー因子行列\n    Q : numpy.ndarray\n        アイテム因子行列\n    \n    Returns:\n    --------\n    float\n        損失関数の値\n    \"\"\"\n    R_hat = P @ Q\n    mask = (R != 0)\n    error = R - R_hat\n    loss = np.sum((error[mask]) ** 2)\n    return loss\n\n# テスト\nloss = compute_loss(R, P, Q)\nprint(f\"compute_loss関数を定義しました\")\nprint(f\"現在の損失: L = {loss:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Column: ハイパーパラメータ\n\n導出した損失関数について、$\\sum_{d=1}^{2} p_{u,d}q_{d,i}$ には潜在因子を示す $d$ という文字を用いています。この点について思い出していただきたいことがあります。それは、行列 $P$, $Q$ の「形」についてです。\n\n- $R$ は $4 \\times 4$ 行列であることに対し、$P$ は $4 \\times 2$ 行列、$Q$ は $2 \\times 4$ 行列としました\n- $4 \\times 2$ 行列と $2 \\times 4$ 行列の積は $4 \\times 4$ 行列となり、$R$ と同じ形になる\n\n$$P = \\begin{pmatrix} p_{1,1} & p_{1,2} \\\\ p_{2,1} & p_{2,2} \\\\ p_{3,1} & p_{3,2} \\\\ p_{4,1} & p_{4,2} \\end{pmatrix}, \\quad Q = \\begin{pmatrix} q_{1,1} & q_{1,2} & q_{1,3} & q_{1,4} \\\\ q_{2,1} & q_{2,2} & q_{2,3} & q_{2,4} \\end{pmatrix}$$\n\nそもそも、$R$ と同じ形にしたいのであれば、$4 \\times 1$ 行列と $1 \\times 4$ 行列の積や、$4 \\times 3$ 行列と $3 \\times 4$ 行列の積でも問題はありません。\n\n行列因子分解において、先程の $d$ に該当する部分は、**数理モデルの設計者が定める値**です。このような値のことを**ハイパーパラメータ**と呼びます。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ハイパーパラメータ d の影響\nprint(\"=== ハイパーパラメータ d の影響 ===\")\nprint()\n\nprint(\"潜在因子数 d を変えると、P, Q の行列サイズが変わります:\")\nprint()\n\nfor d in [1, 2, 3, 4]:\n    print(f\"d = {d}:\")\n    print(f\"  P: 4×{d} 行列 ({4*d} 個のパラメータ)\")\n    print(f\"  Q: {d}×4 行列 ({d*4} 個のパラメータ)\")\n    print(f\"  合計: {4*d + d*4} 個のパラメータ\")\n    print()\n\nprint(\"d が大きいほど表現力は高まるが、過学習のリスクも高まる。\")\nprint(\"d が小さいほど汎化性能は高まるが、表現力が不足する可能性がある。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-15 損失関数を最適化する ー最小二乗法と微分・偏微分ー\n\n今一度、目的に立ち返りましょう。ここでの目的は損失関数 $L$ を最小化することです。\n\n## 目的の書き換えの流れ\n\n$$R = PQ \\text{ となるような潜在因子行列 } P, Q \\text{ を見つける}$$\n$$\\downarrow$$\n$$\\text{残差行列 } E \\text{ を最小化する}$$\n$$\\downarrow$$\n$$\\text{残差行列 } E \\text{ の各要素を2乗し、それらの総和（}\\sum\\text{）を計算した結果が}$$\n$$\\text{最小となる行列 } P, Q \\text{ を見つける}$$\n\nこの「各要素を2乗し、それらの総和の値を最小化する」アプローチを**最小二乗法**と呼びます。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 損失関数のグラフイメージ\n\n式 (2-26) で2乗されている要素 $\\{r_{u,i} - (\\sum_{d=1}^{2} p_{u,d}q_{d,i})\\}^2$ の中身は行列 $P$, $Q$ のそれぞれの要素であり、$p_{u,d}$, $q_{d,i}$ は1次の項です。\n\nということは、$\\{r_{u,i} - (\\sum_{d=1}^{2} p_{u,d}q_{d,i})\\}^2$ の計算結果は**2次以上の多項式**だと考えられます。\n\nつまり、この数式をグラフにすると、**曲線**あるいは**曲面**を描くはずです。\n\n損失関数 $L$ の中身に注目して式 (2-26') を定義しましょう：\n\n$$l = \\left\\{r_{u,i} - \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)\\right\\}^2$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.40: 損失関数のイメージ（2次関数）\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左: 1変数の場合（放物線）\nax1 = axes[0]\nx = np.linspace(-3, 3, 100)\ny = x ** 2\nax1.plot(x, y, 'b-', linewidth=2)\nax1.axhline(y=0, color='k', linewidth=0.5)\nax1.axvline(x=0, color='k', linewidth=0.5)\n\n# 最小点を示す\nax1.plot(0, 0, 'ro', markersize=10, label='最小点')\nax1.annotate('最小値', xy=(0, 0), xytext=(0.5, 2),\n            arrowprops=dict(arrowstyle='->', color='red'),\n            fontsize=12, color='red')\n\nax1.set_xlabel('パラメータ値', fontsize=12)\nax1.set_ylabel('損失 l', fontsize=12)\nax1.set_title('1変数の場合：放物線', fontsize=12)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 右: 2変数の場合（曲面）\nax2 = axes[1]\nfrom mpl_toolkits.mplot3d import Axes3D\nax2 = fig.add_subplot(122, projection='3d')\n\nx = np.linspace(-2, 2, 50)\ny = np.linspace(-2, 2, 50)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\nax2.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax2.scatter([0], [0], [0], color='red', s=100, label='最小点')\n\nax2.set_xlabel('パラメータ1')\nax2.set_ylabel('パラメータ2')\nax2.set_zlabel('損失 L')\nax2.set_title('2変数の場合：曲面（お椀型）', fontsize=12)\n\nplt.suptitle('図2.40: 損失関数のグラフイメージ', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"損失関数は「お椀」のような形をしており、最も低い点（最小値）を探すのが目標です。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 微分とは\n\nここで、行列 $P$, $Q$ のそれぞれの要素 $p_{u,d}$, $q_{d,i}$ は、現時点ではどのような値なのかわからない**未知数**です。\n\n$l = \\{r_{u,i} - (\\sum_{d=1}^{2} p_{u,d}q_{d,i})\\}^2$ という数式は2つの未知数 $p_{u,d}$, $q_{d,i}$ を含む**関数**だと言えます。\n\nグラフの「底」の部分こそが、求めたい最小値だと言えます。しかし、グラフで描写して人間が目で見たなら「ここが最小値だ」と容易に判断できますが、このような判断をコンピュータに実行させる場合、目で見て判断するといった手法は採用できません。\n\n**ここで登場するのが微分です。** 微分とは、簡単に言えばある関数の**傾き**を計算する手法です。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Lesson: 微分\n\n微分とは、数学及び物理学史上の天才として名高い**アイザック・ニュートン**（Sir Isaac Newton, 1642 - 1727）と**ゴットフリート・ライプニッツ**（Gottfried Wilhelm Leibniz, 1646 - 1716）によって発明・確立された数理的手法です。\n\n微分は変化の「瞬間」を捉え、解析することを可能にします。\n\n### 微分係数と導関数\n\n- **微分係数**とは、先述した「傾き」を示す具体的な「値」のこと\n- ある関数 $y = f(x)$ において、$x = a$ から $x = a + h$ に変化したとき、$y = f(x)$ の値がどのように変化するのかを考える\n\n$$f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}$$\n\n### 平均変化率と微分係数\n\n図2.42で、$x = a$ から $x = a + h$ に変化したとき $y$ の値も $f(a)$ から $f(a+h)$ に変化しています。このとき、$y = f(x)$ の**平均変化率**は $\\frac{f(a+h) - f(a)}{h}$ と表すことができます。\n\n微分係数を求めるということは、この $x$, $y$ の増加率の双方に影響している $h$ を限りなく0に近づけるということです。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.42, 2.43: 微分係数のイメージ\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左: 平均変化率\nax1 = axes[0]\nx = np.linspace(0, 3, 100)\ny = 0.5 * x ** 2\nax1.plot(x, y, 'b-', linewidth=2, label='$y = f(x)$')\n\n# 点aとa+hを示す\na = 1.0\nh = 1.0\nfa = 0.5 * a ** 2\nfah = 0.5 * (a + h) ** 2\n\nax1.plot([a, a], [0, fa], 'g--', alpha=0.5)\nax1.plot([a+h, a+h], [0, fah], 'g--', alpha=0.5)\nax1.plot([a, a+h], [fa, fa], 'r--', alpha=0.5, label='$x$の増加量 $h$')\nax1.plot([a+h, a+h], [fa, fah], 'orange', linestyle='--', alpha=0.5, label='$y$の増加量 $f(a+h)-f(a)$')\n\n# 割線\nax1.plot([a, a+h], [fa, fah], 'r-', linewidth=2, label='割線')\nax1.plot(a, fa, 'ko', markersize=8)\nax1.plot(a+h, fah, 'ko', markersize=8)\n\nax1.annotate('$a$', (a, -0.2), fontsize=12, ha='center')\nax1.annotate('$a+h$', (a+h, -0.2), fontsize=12, ha='center')\nax1.annotate('$f(a)$', (a-0.15, fa), fontsize=10, ha='right')\nax1.annotate('$f(a+h)$', (a+h+0.1, fah), fontsize=10, ha='left')\n\nax1.set_xlim(-0.2, 3)\nax1.set_ylim(-0.5, 5)\nax1.set_xlabel('$x$', fontsize=12)\nax1.set_ylabel('$y$', fontsize=12)\nax1.set_title('図2.42: 平均変化率', fontsize=12)\nax1.legend(loc='upper left')\nax1.grid(True, alpha=0.3)\n\n# 右: h→0の極限（接線）\nax2 = axes[1]\nax2.plot(x, y, 'b-', linewidth=2, label='$y = f(x)$')\n\n# 接線\na = 1.0\nfa = 0.5 * a ** 2\nslope = a  # f'(x) = x なので x=1 での傾きは1\n\nx_tangent = np.linspace(0, 2.5, 100)\ny_tangent = slope * (x_tangent - a) + fa\nax2.plot(x_tangent, y_tangent, 'r-', linewidth=2, label=f'接線 (傾き={slope})')\nax2.plot(a, fa, 'ro', markersize=10)\n\nax2.annotate('接点', (a, fa), xytext=(a+0.3, fa+0.5),\n            arrowprops=dict(arrowstyle='->', color='red'),\n            fontsize=12, color='red')\n\nax2.set_xlim(-0.2, 3)\nax2.set_ylim(-0.5, 5)\nax2.set_xlabel('$x$', fontsize=12)\nax2.set_ylabel('$y$', fontsize=12)\nax2.set_title('図2.43: $h$→0の極限で接線になる', fontsize=12)\nax2.legend(loc='upper left')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"微分係数 f'(a) は、x = a における接線の傾きを表します。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 微分係数の計算例\n\n$f(x) = \\frac{1}{2}x^2 + 1$ について、$x = 1$ における微分係数 $f'(1)$ を求めてみましょう。\n\n$$f'(1) = \\lim_{h \\to 0} \\frac{f(1+h) - f(1)}{h}$$\n\n$$= \\lim_{h \\to 0} \\frac{\\{\\frac{1}{2}(1+h)^2 + 1\\} - \\{\\frac{1}{2}(1)^2 + 1\\}}{h}$$\n\n$$= \\lim_{h \\to 0} \\frac{\\{\\frac{1}{2} + h + \\frac{1}{2}h^2 + 1\\} - \\frac{3}{2}}{h}$$\n\n$$= \\lim_{h \\to 0} \\frac{\\frac{1}{2}h^2 + h}{h}$$\n\n$$= \\lim_{h \\to 0} \\left(\\frac{1}{2}h + 1\\right) = 1$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.44: 微分係数及び接線の方程式の導出例\nfig, ax = plt.subplots(figsize=(8, 6))\n\nx = np.linspace(0, 2.5, 100)\ny = 0.5 * x ** 2 + 1\n\nax.plot(x, y, 'b-', linewidth=2, label='$y = \\\\frac{1}{2}x^2 + 1$')\n\n# x=1での接線\na = 1.0\nfa = 0.5 * a**2 + 1  # = 1.5\nslope = 1  # f'(1) = 1\n\nx_tangent = np.linspace(0, 2.5, 100)\ny_tangent = slope * (x_tangent - a) + fa\nax.plot(x_tangent, y_tangent, 'r-', linewidth=2, label='$y = x + \\\\frac{1}{2}$（接線）')\n\nax.plot(a, fa, 'ro', markersize=10)\nax.annotate(f'接点 (1, {fa})', (a, fa), xytext=(1.3, 2.2),\n            arrowprops=dict(arrowstyle='->', color='red'),\n            fontsize=11, color='red')\n\nax.set_xlim(-0.2, 2.5)\nax.set_ylim(0, 4)\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_title('図2.44: 微分係数及び接線の方程式の導出例', fontsize=12)\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"f'(1) = 1 より、x=1 における接線の傾きは 1\")\nprint(\"接線の方程式: y = x + 1/2\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 微分係数と導関数の違い\n\nここまでの解説で示した通り、微分係数とは接線の傾きであり、具体的な「値」を示すものです。一方、「微分する」とは「微分係数を求める」というよりも、この傾きそのものを「関数として求める」ことを指すことが通常です。その関数のことを**導関数**と呼びます。\n\n$$\\text{微分係数}: f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}$$\n\n$$\\text{導関数}: f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n\n着目すべきは、定数 $a$ と変数 $x$ のどちらが使われているかという点です。\n\n- **微分係数**は定数 $a$ が使われているので、計算結果は当然ながら**定数**となる\n- **導関数**は変数 $x$ が用いられているため、計算結果は変数 $x$ が含まれる**関数**となる",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Lesson: 微分・偏微分の計算例\n\n実際の微分計算では以下の公式に当てはめて考えることが一般的であり、計算も簡単になります。\n\n$$(x^n)' = nx^{n-1}$$\n\n### 具体的な計算例\n\n$f(x) = x$, $g(x) = 3x^2$, $h(x) = 4x^3$ をそれぞれ $x$ について微分すると：\n\n$$\\frac{d}{dx}f(x) = 1 \\times x^{1-1} = 1 \\times x^0 = 1$$\n\n$$\\frac{d}{dx}g(x) = 2 \\times 3 \\times x^{2-1} = 6 \\times x^1 = 6x$$\n\n$$\\frac{d}{dx}h(x) = 3 \\times 4 \\times x^{3-1} = 12 \\times x^2 = 12x^2$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 微分の計算例\nprint(\"=== 微分の計算例 ===\")\nprint()\n\nprint(\"公式: (x^n)' = n × x^(n-1)\")\nprint()\n\n# 関数と導関数のペア\nfunctions = [\n    (\"f(x) = x\", \"f'(x) = 1\"),\n    (\"g(x) = 3x²\", \"g'(x) = 6x\"),\n    (\"h(x) = 4x³\", \"h'(x) = 12x²\"),\n]\n\nfor f, df in functions:\n    print(f\"{f:20} → {df}\")\n\nprint()\n\n# yで微分した場合（yを含まない関数）\nprint(\"y について微分した場合（y を含まない関数）:\")\nprint(\"d/dy f(x) = 0,  d/dy g(x) = 0,  d/dy h(x) = 0\")\nprint()\nprint(\"→ 微分する変数と関係ない変数だけの項や定数項は、微分すると0になる\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 偏微分\n\n$f(x, y) = 3x^2 + 2y + 1$ という2変数関数を $x$ について微分します。$x$ を含む項は1項目の $3x^2$ のみなので次のように計算されます。\n\n$$\\frac{\\partial}{\\partial x}f(x,y) = 2 \\times 3 \\times x^{2-1} = 6x$$\n\nなお、$\\frac{\\partial}{\\partial x}$ は多変数関数に対して1つの変数で微分する**偏微分**を示す微分記号です。\n\n$f(x, y) = 3x^2 + 2y + 1$ を $y$ について微分すると、$y$ を含む項は2項目の $2y$ のみなので次のように計算されます：\n\n$$\\frac{\\partial}{\\partial y}f(x,y) = 2 \\times y^{1-1} = 2$$\n\n**つまり、微分する変数と関係ない変数だけの項や定数項は、微分すると0になるということです。**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 偏微分の計算例\nprint(\"=== 偏微分の計算例 ===\")\nprint()\n\nprint(\"f(x, y) = 3x² + 2y + 1\")\nprint()\n\nprint(\"x で偏微分:\")\nprint(\"  ∂f/∂x = 6x\")\nprint(\"  （2y + 1 は x を含まないので 0 になる）\")\nprint()\n\nprint(\"y で偏微分:\")\nprint(\"  ∂f/∂y = 2\")\nprint(\"  （3x² + 1 は y を含まないので 0 になる）\")\nprint()\n\nprint(\"=== 偏微分のポイント ===\")\nprint(\"- 偏微分する変数以外は「定数」として扱う\")\nprint(\"- 定数を微分すると 0 になる\")\nprint(\"- 偏微分記号 ∂ は「パーシャル」と読む\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 傾きが0のとき最小値を取る\n\n以上を踏まえ、式 (2-26') の**接線**について考えてみましょう。グラフの接線を図2.45のように図示すると、グラフと接線の**接点**ごとに**傾き**の大きさが異なることがわかります。\n\nこの傾きが、微分によって計算される値です。図を見ればわかる通り、**傾きが0となったとき、最小値を取ります。**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.45: 傾きと最小値の関係\nfig, ax = plt.subplots(figsize=(10, 6))\n\nx = np.linspace(-2, 2, 100)\ny = x ** 2\n\nax.plot(x, y, 'b-', linewidth=2, label='$l = f(p, q)$')\n\n# 複数の接点での接線\npoints = [(-1.5, (-1.5)**2), (-0.5, (-0.5)**2), (0, 0), (0.5, 0.5**2), (1.5, 1.5**2)]\nlabels = ['傾き:大(負)', '傾き:小(負)', '傾き:最小(0)', '傾き:小(正)', '傾き:大(正)']\ncolors = ['orange', 'gold', 'red', 'gold', 'orange']\n\nfor (px, py), label, color in zip(points, labels, colors):\n    slope = 2 * px  # y = x^2 の導関数は 2x\n    x_tan = np.linspace(px - 0.8, px + 0.8, 50)\n    y_tan = slope * (x_tan - px) + py\n    ax.plot(x_tan, y_tan, '-', color=color, linewidth=2, alpha=0.8)\n    ax.plot(px, py, 'o', color=color, markersize=8)\n    \n    if px == 0:\n        ax.annotate(label, (px, py), xytext=(px + 0.3, py + 0.5),\n                   fontsize=10, color='red', fontweight='bold',\n                   arrowprops=dict(arrowstyle='->', color='red'))\n    elif px < 0:\n        ax.annotate(label, (px, py), xytext=(px - 0.5, py + 1),\n                   fontsize=9, ha='right')\n    else:\n        ax.annotate(label, (px, py), xytext=(px + 0.3, py + 1),\n                   fontsize=9)\n\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\n\nax.set_xlabel('パラメータ値', fontsize=12)\nax.set_ylabel('損失 $l$', fontsize=12)\nax.set_title('図2.45: 曲線を描く「下に凸の関数」は、傾きが0に近づくほど、その点における関数の値は最小に近づく', fontsize=11)\nax.set_xlim(-2.5, 2.5)\nax.set_ylim(-0.5, 4)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-16 計算結果を統合して数理モデルを導出する ー偏微分と総和記号Σー\n\nでは、実際に微分計算を行います。ここで微分対象となるのは損失関数 $L = \\sum_{u=1}^{4} \\sum_{i=1}^{4} \\{r_{u,i} - (\\sum_{d=1}^{2} p_{u,d}q_{d,i})\\}^2$ ではなく、その $\\sum$ の中身の $l = \\{r_{u,i} - (\\sum_{d=1}^{2} p_{u,d}q_{d,i})\\}^2$ であることに留意してください。\n\n## $l$ の展開\n\nまず、$l$ について2乗の計算を行います。\n\n$$l = \\left\\{r_{u,i} - \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)\\right\\}^2 = (r_{u,i})^2 - 2r_{u,i}\\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right) + \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)^2$$\n\nこれは $(a-b)^2 = a^2 - 2ab + b^2$ の形です。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## $l$ の展開（続き）\n\n$\\sum$ があると考えづらいため、和の形に戻してさらに展開します。\n\n$$(r_{u,i})^2 - 2r_{u,i}\\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right) + \\left(\\sum_{d=1}^{2} p_{u,d}q_{d,i}\\right)^2$$\n\n$$= (r_{u,i})^2 - 2r_{u,i}(p_{u,1}q_{1,i} + p_{u,2}q_{2,i}) + (p_{u,1}q_{1,i} + p_{u,2}q_{2,i})^2$$\n\n$$= (r_{u,i})^2 - 2r_{u,i} \\cdot p_{u,1}q_{1,i} - 2r_{u,i} \\cdot p_{u,2}q_{2,i} + (p_{u,1})^2(q_{1,i})^2 + 2p_{u,1}q_{1,i} \\cdot p_{u,2}q_{2,i} + (p_{u,2})^2(q_{2,i})^2$$\n\n数式を展開した結果、4つの未知数 $p_{u,1}$, $p_{u,2}$, $q_{1,i}$, $q_{2,i}$ が現れました。この数式を、それぞれの未知数で微分することで $l$ を最適化します。\n\nなお、複数の未知数が存在する数式に対して1つの未知数で微分することを**偏微分**と呼びます。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## $p_{u,1}$ での偏微分\n\n展開した式について、$p_{u,1}$ で偏微分計算を行います。これにより、未知数ごとに $l$ を最適化できます。$p_{u,1}$ に関係のない項は消えていることを確認してください。\n\n$$\\frac{\\partial l}{\\partial p_{u,1}} = -2r_{u,i} \\cdot q_{1,i} + 2(q_{1,i})^2 p_{u,1} + 2q_{1,i} \\cdot p_{u,2}q_{2,i}$$\n\n$$= -2r_{u,i} \\cdot q_{1,i} + 2q_{1,i}(p_{u,1}q_{1,i} + p_{u,2}q_{2,i})$$\n\n$$= -2\\{r_{u,i} - (p_{u,1}q_{1,i} + p_{u,2}q_{2,i})\\}q_{1,i}$$\n\n## 損失関数の偏微分の一般形\n\n同様に他の未知数 $p_{u,2}$, $q_{1,i}$, $q_{2,i}$ でも偏微分計算を行います。比較しやすいように $\\frac{\\partial l}{\\partial p_{u,1}}$ の計算結果も再掲します。\n\n$$\\frac{\\partial L}{\\partial p_{u,k}} = -2\\sum_{i=1}^{4} e_{u,i}q_{k,i}$$\n\n$$\\frac{\\partial L}{\\partial q_{k,i}} = -2\\sum_{u=1}^{4} e_{u,i}p_{u,k}$$\n\nただし、$e_{u,i} = r_{u,i} - \\sum_{d=1}^{2} p_{u,d}q_{d,i}$（残差）",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 偏微分の計算結果の確認\nprint(\"=== 損失関数の偏微分 ===\")\nprint()\n\nprint(\"残差: e_{u,i} = r_{u,i} - Σ p_{u,d}q_{d,i}\")\nprint()\n\nprint(\"P の要素 p_{u,k} での偏微分:\")\nprint(\"  ∂L/∂p_{u,k} = -2 Σ e_{u,i} × q_{k,i}\")\nprint(\"             = -2 × (残差) × (対応するQの要素)\")\nprint()\n\nprint(\"Q の要素 q_{k,i} での偏微分:\")\nprint(\"  ∂L/∂q_{k,i} = -2 Σ e_{u,i} × p_{u,k}\")\nprint(\"             = -2 × (残差) × (対応するPの要素)\")\nprint()\n\nprint(\"=== 偏微分の意味 ===\")\nprint(\"これらの偏微分結果が「勾配」を表します。\")\nprint(\"勾配の方向に沿ってパラメータを更新することで、損失を減らせます。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-17 更新式を設計して予測値を推計する ー勾配降下法ー\n\n本章の総仕上げです。前節で導出した式 (2-31) を用いて、勾配降下法によって損失関数を最小化するために、ユーザー因子行列及びアイテム因子行列の各要素の**更新式**を定義します。\n\n## 情報工学的アプローチ\n\n| 損失関数の偏微分計算 | 勾配降下法によるユーザー因子行列/アイテム因子行列の各要素の更新式 |\n|:---|:---|\n| $\\frac{\\partial L}{\\partial p_{u,k}} = -2\\sum_{i=1}^{4} e_{u,i}q_{k,i}$ | $p_{u,k}^{(n)} = p_{u,k}^{(n-1)} + 2\\eta \\sum_{i=1}^{4} e_{u,i}q_{k,i}^{(n-1)}$ |\n| $\\frac{\\partial L}{\\partial q_{k,i}} = -2\\sum_{u=1}^{4} e_{u,i}p_{u,k}$ | $q_{k,i}^{(n)} = q_{k,i}^{(n-1)} + 2\\eta \\sum_{u=1}^{4} e_{u,i}p_{u,k}^{(n-1)}$ |\n\nここで：\n- $n$: 自然数（更新回数）\n- $p_{u,k}^{(n)}$: $n$回更新後のユーザー因子行列 $P$ の $u$ 行 $k$ 列目の要素\n- $q_{k,i}^{(n)}$: $n$回更新後のアイテム因子行列 $Q$ の $k$ 行 $i$ 列目の要素",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 傾きの方向に応じた更新\n\n$\\frac{\\partial L}{\\partial p_{u,k}}$ 及び $\\frac{\\partial L}{\\partial q_{k,i}}$ の値が0より大きい、または小さければ最小化の余地があることを意味します。\n\n- **傾きが正（右上がり）の場合**：$p_{u,k}$ の値を**小さくする**\n- **傾きが負（右下がり）の場合**：$p_{u,k}$ の値を**大きくする**\n\nこれにより、接線の傾きを0に近づければよいことになります。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.48, 2.49: 傾きの方向に応じた更新\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nx = np.linspace(-2, 2, 100)\ny = x ** 2\n\n# 左：傾きが正の場合 → 値を小さくする\nax1 = axes[0]\nax1.plot(x, y, 'b-', linewidth=2)\n\n# 現在位置（傾きが正）\np0 = 1.5\nax1.plot(p0, p0**2, 'ro', markersize=12, label='現在位置 $p_{u,k}^{(0)}$')\nax1.axvline(x=p0, color='r', linestyle='--', alpha=0.5)\n\n# 更新後の位置\np1 = 0.8\nax1.plot(p1, p1**2, 'go', markersize=12, label='更新後 $p_{u,k}^{(1)}$')\nax1.axvline(x=p1, color='g', linestyle='--', alpha=0.5)\n\n# 矢印\nax1.annotate('', xy=(p1, 1), xytext=(p0, 1),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\nax1.text((p0+p1)/2, 1.3, '値を小さくする', ha='center', fontsize=10, color='purple')\n\nax1.set_xlabel('$p_{u,k}$', fontsize=12)\nax1.set_ylabel('損失 $L$', fontsize=12)\nax1.set_title('図2.48: 傾きが正の方向であれば、$p_{u,k}$の値を小さくする', fontsize=11)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 右：傾きが負の場合 → 値を大きくする\nax2 = axes[1]\nax2.plot(x, y, 'b-', linewidth=2)\n\n# 現在位置（傾きが負）\np0 = -1.5\nax2.plot(p0, p0**2, 'ro', markersize=12, label='現在位置 $p_{u,k}^{(0)}$')\nax2.axvline(x=p0, color='r', linestyle='--', alpha=0.5)\n\n# 更新後の位置\np1 = -0.8\nax2.plot(p1, p1**2, 'go', markersize=12, label='更新後 $p_{u,k}^{(1)}$')\nax2.axvline(x=p1, color='g', linestyle='--', alpha=0.5)\n\n# 矢印\nax2.annotate('', xy=(p1, 1), xytext=(p0, 1),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\nax2.text((p0+p1)/2, 1.3, '値を大きくする', ha='center', fontsize=10, color='purple')\n\nax2.set_xlabel('$p_{u,k}$', fontsize=12)\nax2.set_ylabel('損失 $L$', fontsize=12)\nax2.set_title('図2.49: 傾きが負の方向であれば、$p_{u,k}$の値を大きくする', fontsize=11)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 更新式の導出\n\nこれらの計算を実現する数理モデルを設計します。偏微分した計算結果を活用し、$p_{u,k}$ の初期値を $p_{u,k}^{(0)}$、移動後の $p_{u,k}$ の値を $p_{u,k}^{(1)}$ とすると：\n\n$$p_{u,k}^{(1)} = p_{u,k}^{(0)} - \\eta \\cdot \\frac{\\partial L}{\\partial p_{u,k}^{(0)}}$$\n\n$$= p_{u,k}^{(0)} - \\eta \\left(-2\\sum_{i=1}^{4} e_{u,i}q_{k,i}^{(0)}\\right)$$\n\n$$= p_{u,k}^{(0)} + 2\\eta \\sum_{i=1}^{4} e_{u,i}q_{k,i}^{(0)}$$\n\n## 学習率 η（イータ）\n\nここで、$\\eta$（eta：イータ）は**学習率**と呼ばれるハイパーパラメータで、偏微分した値をどれだけ反映させるかを調整する役割を果たします。\n\n図2.50に示すように、偏微分した計算結果の値が大き過ぎると、学習率 $\\eta$ を用いない場合、最小値となる $p_{u,k}$ の値を飛び越えてしまう可能性があります。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 図2.50: 学習率ηの必要性\nfig, ax = plt.subplots(figsize=(10, 6))\n\nx = np.linspace(-2, 3, 100)\ny = x ** 2\n\nax.plot(x, y, 'b-', linewidth=2)\n\n# 現在位置\np0 = 2.0\nax.plot(p0, p0**2, 'ro', markersize=12, label='$p_{u,k}^{(0)}$')\nax.axvline(x=p0, color='r', linestyle='--', alpha=0.5)\n\n# 最小値\nax.plot(0, 0, 'g*', markersize=15, label='最小値')\nax.axvline(x=0, color='g', linestyle='--', alpha=0.5)\n\n# 学習率なしで飛び越えてしまう場合\np1_bad = -1.5\nax.plot(p1_bad, p1_bad**2, 'mo', markersize=12, label='学習率なしで飛び越え')\nax.annotate('', xy=(p1_bad, 0.5), xytext=(p0, 0.5),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n\n# 警告テキスト\nax.text(0.25, 3, '学習率ηを用いないと\\n最小値を飛び越えてしまう\\n可能性がある', \n        fontsize=11, ha='center', \n        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nax.set_xlabel('$p_{u,k}$', fontsize=12)\nax.set_ylabel('損失 $L$', fontsize=12)\nax.set_title('図2.50: 学習率ηの必要性', fontsize=12)\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"学習率 η は通常 0.1〜0.001 程度の値で設定されることが多い。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 一般化した更新式\n\n$n$ を自然数とすると、$n$ 回の更新を行った $p_{u,k}^{(n)}$ と $q_{k,i}^{(n)}$ は次の数式で計算されます。これが、ユーザー因子行列及びアイテム因子行列の各要素について、一般化した更新式です。\n\n$$p_{u,k}^{(n)} = p_{u,k}^{(n-1)} + 2\\eta \\sum_{i=1}^{4} e_{u,i}q_{k,i}^{(n-1)}$$\n\n$$q_{k,i}^{(n)} = q_{k,i}^{(n-1)} + 2\\eta \\sum_{u=1}^{4} e_{u,i}p_{u,k}^{(n-1)}$$\n\nこの試行を繰り返すことで、$p_{u,k}$, $q_{k,i}$ の値を最適化します。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 2-18 勾配降下法の計算例\n\nここまで導出してきた内容をもとに、実際に勾配降下法の計算を実行してみましょう。通常はプログラミングによって計算を実行しますが、本書では具体的な計算プロセスをできる限り記述することで、数理的な理解を目指します。\n\nここでも、具体例として度々登場している以下の評価値行列（式 (2-16)）を用います。\n\n$$R = \\begin{pmatrix} 2 & 3 & 0 & 5 \\\\ 2 & 5 & 0 & 5 \\\\ 0 & 3 & 4 & 4 \\\\ 4 & 2 & 3 & 0 \\end{pmatrix}$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 勾配降下法の計算例\nprint(\"=== 勾配降下法の計算例 ===\")\nprint()\n\n# 評価値行列\nR = np.array([\n    [2, 3, 0, 5],\n    [2, 5, 0, 5],\n    [0, 3, 4, 4],\n    [4, 2, 3, 0]\n])\n\nprint(\"評価値行列 R:\")\nprint(R)\nprint()\n\n# 初期値（教科書の例に合わせる）\nP = np.array([\n    [0.37, 0.95],\n    [0.73, 0.60],\n    [0.16, 0.16],\n    [0.06, 0.87]\n])\n\nQ = np.array([\n    [0.60, 0.71, 0.02, 0.97],\n    [0.83, 0.21, 0.18, 0.18]\n])\n\nprint(\"初期ユーザー因子行列 P:\")\nprint(P)\nprint()\n\nprint(\"初期アイテム因子行列 Q:\")\nprint(Q)\nprint()\n\n# 予測行列 R̂ = PQ\nR_hat = P @ Q\nprint(\"予測行列 R̂ = PQ:\")\nprint(np.round(R_hat, 2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 残差行列と損失関数の計算\nprint(\"=== 残差行列 E ===\")\nprint()\n\n# 残差行列\nE = R - R_hat\nprint(\"E = R - R̂:\")\nprint(np.round(E, 2))\nprint()\n\n# 損失関数（欠損値を除く）\nmask = (R != 0)\nloss = np.sum((E[mask]) ** 2)\nprint(f\"初期損失: L = {loss:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 行列因子分解クラスの実装\nclass MatrixFactorization:\n    \"\"\"\n    行列因子分解による協調フィルタリング\n    \n    Parameters:\n    -----------\n    n_factors : int\n        潜在因子の数（ハイパーパラメータ d）\n    learning_rate : float\n        学習率 η\n    n_epochs : int\n        更新回数\n    \"\"\"\n    \n    def __init__(self, n_factors=2, learning_rate=0.01, n_epochs=100):\n        self.n_factors = n_factors\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.P = None\n        self.Q = None\n        self.loss_history = []\n    \n    def fit(self, R):\n        \"\"\"\n        評価値行列 R から P, Q を学習する\n        \"\"\"\n        n_users, n_items = R.shape\n        \n        # 初期値をランダムに設定\n        np.random.seed(42)\n        self.P = np.random.rand(n_users, self.n_factors)\n        self.Q = np.random.rand(self.n_factors, n_items)\n        \n        # マスク行列（欠損値の位置）\n        mask = (R != 0)\n        \n        # 勾配降下法による更新\n        for epoch in range(self.n_epochs):\n            # 予測値と残差\n            R_hat = self.P @ self.Q\n            E = R - R_hat\n            \n            # 損失を記録\n            loss = np.sum((E[mask]) ** 2)\n            self.loss_history.append(loss)\n            \n            # P の更新\n            for u in range(n_users):\n                for k in range(self.n_factors):\n                    gradient = 0\n                    for i in range(n_items):\n                        if mask[u, i]:\n                            gradient += E[u, i] * self.Q[k, i]\n                    self.P[u, k] += 2 * self.learning_rate * gradient\n            \n            # Q の更新\n            for k in range(self.n_factors):\n                for i in range(n_items):\n                    gradient = 0\n                    for u in range(n_users):\n                        if mask[u, i]:\n                            gradient += E[u, i] * self.P[u, k]\n                    self.Q[k, i] += 2 * self.learning_rate * gradient\n        \n        return self\n    \n    def predict(self):\n        \"\"\"\n        予測行列 R̂ = PQ を返す\n        \"\"\"\n        return self.P @ self.Q\n    \n    def plot_loss(self):\n        \"\"\"\n        損失の推移をプロット\n        \"\"\"\n        plt.figure(figsize=(10, 5))\n        plt.plot(self.loss_history, 'b-', linewidth=2)\n        plt.xlabel('エポック数', fontsize=12)\n        plt.ylabel('損失 L', fontsize=12)\n        plt.title('勾配降下法による損失の推移', fontsize=14)\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\nprint(\"MatrixFactorizationクラスを定義しました\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 行列因子分解の実行\nprint(\"=== 行列因子分解の実行 ===\")\nprint()\n\n# モデルの作成と学習\nmf = MatrixFactorization(n_factors=2, learning_rate=0.01, n_epochs=500)\nmf.fit(R)\n\nprint(f\"学習完了！\")\nprint(f\"初期損失: {mf.loss_history[0]:.4f}\")\nprint(f\"最終損失: {mf.loss_history[-1]:.4f}\")\nprint()\n\n# 損失の推移をプロット\nmf.plot_loss()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 学習結果の確認\nprint(\"=== 学習結果 ===\")\nprint()\n\nprint(\"学習後のユーザー因子行列 P:\")\nprint(np.round(mf.P, 3))\nprint()\n\nprint(\"学習後のアイテム因子行列 Q:\")\nprint(np.round(mf.Q, 3))\nprint()\n\n# 予測行列\nR_hat = mf.predict()\nprint(\"予測行列 R̂ = PQ:\")\nprint(np.round(R_hat, 2))\nprint()\n\nprint(\"元の評価値行列 R:\")\nprint(R)\nprint()\n\nprint(\"差分（既知の評価値のみ）:\")\nE = R - R_hat\nE_display = np.where(R != 0, np.round(E, 2), '-')\nprint(E_display)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 欠損値の予測結果を可視化\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左：元の評価値行列\nax1 = axes[0]\nim1 = ax1.imshow(np.where(R == 0, np.nan, R), cmap='YlOrRd', vmin=1, vmax=5)\nax1.imshow(np.where(R == 0, 1, np.nan), cmap='gray', vmin=0, vmax=1, alpha=0.3)\n\nfor i in range(4):\n    for j in range(4):\n        if R[i, j] == 0:\n            ax1.text(j, i, '?', ha='center', va='center', fontsize=14, color='gray')\n        else:\n            ax1.text(j, i, str(R[i, j]), ha='center', va='center', fontsize=14)\n\nax1.set_xticks(range(4))\nax1.set_yticks(range(4))\nax1.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\nax1.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\nax1.set_title('元の評価値行列 R（?=欠損値）')\n\n# 右：予測行列\nax2 = axes[1]\nR_hat = mf.predict()\nim2 = ax2.imshow(R_hat, cmap='YlOrRd', vmin=1, vmax=5)\n\nfor i in range(4):\n    for j in range(4):\n        if R[i, j] == 0:\n            ax2.text(j, i, f'{R_hat[i,j]:.1f}', ha='center', va='center', \n                    fontsize=12, color='blue', fontweight='bold')\n        else:\n            ax2.text(j, i, f'{R_hat[i,j]:.1f}', ha='center', va='center', fontsize=12)\n\nax2.set_xticks(range(4))\nax2.set_yticks(range(4))\nax2.set_xticklabels(['item1', 'item2', 'item3', 'item4'])\nax2.set_yticklabels(['user1', 'user2', 'user3', 'user4'])\nax2.set_title('予測行列 R̂ = PQ（青字=欠損値の予測）')\nplt.colorbar(im2, ax=ax2, label='評価値')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"欠損値の予測結果:\")\nprint(f\"  user1のitem3: {R_hat[0, 2]:.2f}\")\nprint(f\"  user2のitem3: {R_hat[1, 2]:.2f}\")\nprint(f\"  user3のitem1: {R_hat[2, 0]:.2f}\")\nprint(f\"  user4のitem4: {R_hat[3, 3]:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 第2章 まとめ\n\n## 本章で学んだ主要な概念\n\n### 1. 評価値行列 R\n- ユーザー×アイテムの行列で評価値を表現\n- 0は欠損値（未評価）を示す\n\n### 2. 協調フィルタリング\n- **ユーザーベース**: 類似ユーザーの評価から予測\n- **アイテムベース**: 類似アイテムの評価から予測\n- コサイン類似度で「似ている」を定量化\n\n### 3. コサイン類似度\n$$\\cos(a, b) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}$$\n- 中心化により評価のバイアスを除去\n- 指示関数で欠損値を除外\n\n### 4. 行列因子分解（Matrix Factorization）\n$$R \\approx P \\times Q$$\n- $P$: ユーザー因子行列（$U \\times d$）\n- $Q$: アイテム因子行列（$d \\times I$）\n- $d$: 潜在因子数（ハイパーパラメータ）\n\n### 5. 損失関数と最小二乗法\n$$L = \\sum_{u,i} \\left\\{r_{u,i} - \\sum_d p_{u,d}q_{d,i}\\right\\}^2$$\n- 二乗することで正負の打ち消しを防ぐ\n\n### 6. 勾配降下法\n$$p_{u,k}^{(n)} = p_{u,k}^{(n-1)} + 2\\eta \\sum_i e_{u,i}q_{k,i}^{(n-1)}$$\n- $\\eta$: 学習率（ハイパーパラメータ）\n- 偏微分で勾配を計算し、パラメータを更新",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 本Notebookで実装した関数・クラスのまとめ\nprint(\"=== 本Notebookで実装した関数・クラス ===\")\nprint()\n\nprint(\"【関数】\")\nprint(\"1. cosine_similarity(a, b)\")\nprint(\"   - 基本的なコサイン類似度\")\nprint()\nprint(\"2. cosine_similarity_centered(a, b)\")\nprint(\"   - 中心化付きコサイン類似度（ユーザーベース）\")\nprint()\nprint(\"3. cosine_similarity_item_based(R, item_i, item_j)\")\nprint(\"   - アイテムベースのコサイン類似度\")\nprint()\nprint(\"4. compute_loss(R, P, Q)\")\nprint(\"   - 二乗誤差の損失関数\")\nprint()\n\nprint(\"【クラス】\")\nprint(\"5. MatrixFactorization\")\nprint(\"   - __init__(n_factors, learning_rate, n_epochs)\")\nprint(\"   - fit(R): 勾配降下法で学習\")\nprint(\"   - predict(): 予測行列を返す\")\nprint(\"   - plot_loss(): 損失の推移をプロット\")\nprint()\n\nprint(\"=== ハイパーパラメータ ===\")\nprint(\"- d (n_factors): 潜在因子の数\")\nprint(\"- η (learning_rate): 学習率\")\nprint(\"- n_epochs: 更新回数\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# まとめ（Part 4）\n\n## 2-15〜2-16で学んだこと\n\n### 微分とは\n- 関数の**傾き**を計算する手法\n- 微分係数 $f'(a)$：具体的な点での傾き（値）\n- 導関数 $f'(x)$：傾きを表す関数\n\n### 微分の公式\n$$(x^n)' = nx^{n-1}$$\n\n### 偏微分\n- 多変数関数に対して1つの変数で微分\n- 他の変数は定数として扱う\n- 記号：$\\frac{\\partial}{\\partial x}$\n\n### 損失関数の偏微分結果\n$$\\frac{\\partial L}{\\partial p_{u,k}} = -2\\sum_{i=1}^{I} e_{u,i}q_{k,i}$$\n\n$$\\frac{\\partial L}{\\partial q_{k,i}} = -2\\sum_{u=1}^{U} e_{u,i}p_{u,k}$$\n\n### 傾きが0のとき最小値\n- 下に凸の関数では、傾き=0 の点で最小値を取る\n- この性質を利用して損失関数を最小化する\n\n## 次のステップ（教科書の続き）\n- 勾配降下法によるパラメータ更新\n- 学習率 $\\eta$ の役割",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# まとめ（Part 3）\n\n## 2-14〜2-15で学んだこと\n\n### 損失関数 L\n残差行列 $E$ のすべての要素の二乗和：\n\n$$L = \\sum_{u=1}^{U} \\sum_{i=1}^{I} \\left\\{r_{u,i} - \\left(\\sum_{d=1}^{D} p_{u,d}q_{d,i}\\right)\\right\\}^2$$\n\n### なぜ二乗するのか\n- 正負の打ち消し合いを防ぐ\n- 大きな誤差にペナルティを与える\n- 微分が容易になる\n\n### 最小二乗法\n「各要素を2乗し、それらの総和の値を最小化する」アプローチ\n\n### ハイパーパラメータ\n- 潜在因子数 $d$ は設計者が決める値\n- $d$ が大きいほど表現力は高いが過学習のリスク\n- $d$ が小さいほど汎化性能は高いが表現力不足の可能性\n\n## 次のステップ（教科書の続き）\n- 微分と偏微分\n- 勾配降下法によるパラメータ更新",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 本Notebookで実装した関数のまとめ\nprint(\"=== 本Notebookで実装した関数 ===\")\nprint()\nprint(\"1. cosine_similarity(a, b)\")\nprint(\"   - 基本的なコサイン類似度を計算\")\nprint()\nprint(\"2. cosine_similarity_centered(a, b)\")\nprint(\"   - ユーザーベース：中心化を行ったコサイン類似度\")\nprint(\"   - 欠損値（0）を自動的に除外\")\nprint()\nprint(\"3. cosine_similarity_item_based(R, item_i, item_j)\")\nprint(\"   - アイテムベース：中心化コサイン類似度\")\nprint(\"   - 列ベクトルを使用\")\nprint()\nprint(\"=== 主な概念 ===\")\nprint()\nprint(\"協調フィルタリング:\")\nprint(\"  - ユーザーベース: 似ているユーザーの評価から予測\")\nprint(\"  - アイテムベース: 似ているアイテムの評価から予測\")\nprint()\nprint(\"行列因子分解:\")\nprint(\"  - R ≈ P × Q に分解\")\nprint(\"  - 潜在因子を通じてユーザーとアイテムの関係を表現\")\nprint(\"  - セレンディピティ（意外な発見）を促進する可能性\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 行列因子分解の具体例\nprint(\"=== 行列因子分解の具体例 ===\")\nprint()\n\n# 評価値行列（5×4）\nR_mf = np.array([\n    [5, 3, 0, 1],\n    [4, 0, 3, 2],\n    [0, 5, 5, 5],\n    [1, 2, 3, 4],\n    [1, 0, 0, 5]\n])\n\nprint(\"評価値行列 R (5ユーザー × 4アイテム):\")\nprint(R_mf)\nprint()\n\n# 仮にP, Qが解析的に求められたとする（d=2）\nP = np.array([\n    [0.3, 2.0],\n    [0.7, 1.5],\n    [1.9, 1.9],\n    [1.6, 0.1],\n    [2.0, 0.1]\n])\n\nQ = np.array([\n    [0.4, 1.2, 1.7, 2.5],\n    [2.4, 1.4, 1.0, 0.2]\n])\n\nprint(\"ユーザー因子行列 P (5×2):\")\nprint(P)\nprint()\n\nprint(\"アイテム因子行列 Q (2×4):\")\nprint(Q)\nprint()\n\n# PQを計算\nPQ = P @ Q\n\nprint(\"予測行列 PQ (5×4):\")\nprint(np.round(PQ, 2))\nprint()\n\nprint(\"元の評価値行列Rと予測行列PQを比較:\")\nprint(\"R で既に評価値がついていた箇所と、PQ の計算結果は非常に近い値となっています。\")\nprint(\"これが行列因子分解の大きな特徴です。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# アイテムベースでの予測値計算\nprint(\"=== r̂_{4,4} の計算（アイテムベース） ===\")\nprint()\n\n# user4の評価値\nr_4_2 = R[3, 1]  # user4のitem2 = 2\nr_4_3 = R[3, 2]  # user4のitem3 = 3\n\ncos_i2_i4 = item_similarity_matrix[1, 3]  # 0.090\ncos_i3_i4 = item_similarity_matrix[2, 3]  # 1.000\n\nprint(\"図2.32: 欠損値の推計で用いる数値\")\nprint(f\"  r₄,₂ = {r_4_2} (user4のitem2評価)\")\nprint(f\"  r₄,₃ = {r_4_3} (user4のitem3評価)\")\nprint(f\"  cos(i₂, i₄) = {cos_i2_i4:.3f}\")\nprint(f\"  cos(i₃, i₄) = {cos_i3_i4:.3f}\")\nprint()\n\n# 予測式: r̂_{4,4} = cos(i2,i4) × r_{4,2} + cos(i3,i4) × r_{4,3}\nr_hat_4_4 = cos_i2_i4 * r_4_2 + cos_i3_i4 * r_4_3\n\nprint(\"計算式:\")\nprint(f\"r̂₄,₄ = cos(i₂, i₄) × r₄,₂ + cos(i₃, i₄) × r₄,₃\")\nprint(f\"     = {cos_i2_i4:.3f} × {r_4_2} + {cos_i3_i4:.3f} × {r_4_3}\")\nprint(f\"     = {cos_i2_i4 * r_4_2:.3f} + {cos_i3_i4 * r_4_3:.3f}\")\nprint(f\"     = {r_hat_4_4:.3f}\")\nprint()\nprint(f\"予測結果: user4のitem4への評価値は {r_hat_4_4:.3f} と推計されました。\")\nprint(\"→ item4はuser4にとっては「可もなく不可もなく」という評価になりそうです。\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-19 数理モデルの違いを俯瞰する\n",
    "## 協調フィルタリングと行列因子分解\n",
    "\n",
    "これまで学んできた3つの手法を比較してみましょう。\n",
    "\n",
    "### ① ユーザーベース協調フィルタリング\n",
    "- **視点**: ミクロ（局所的）\n",
    "- **考え方**: 「このユーザーに似たユーザーは誰か？」を計算\n",
    "- **類似度計算**: ユーザー間のコサイン類似度\n",
    "- **予測方法**: 類似ユーザーの評価値の重み付き平均\n",
    "\n",
    "### ② アイテムベース協調フィルタリング\n",
    "- **視点**: ミクロ（局所的）\n",
    "- **考え方**: 「このアイテムに似たアイテムは何か？」を計算\n",
    "- **類似度計算**: アイテム間のコサイン類似度\n",
    "- **予測方法**: 類似アイテムの評価値の重み付き平均\n",
    "\n",
    "### ③ 行列因子分解\n",
    "- **視点**: マクロ（大域的）\n",
    "- **考え方**: 評価値行列全体を低ランク行列で近似\n",
    "- **最適化**: 損失関数を最小化する P と Q を学習\n",
    "- **予測方法**: $\\hat{R} = P \\times Q$\n",
    "\n",
    "### 図解: ミクロ視点 vs マクロ視点\n",
    "\n",
    "```\n",
    "ミクロ視点（協調フィルタリング）:\n",
    "  ┌─────────────────────────────────────┐\n",
    "  │     R（評価値行列）                  │\n",
    "  │  ┌───┬───┬───┬───┐               │\n",
    "  │  │ 2 │ 3 │ ? │ 5 │ ← User1       │\n",
    "  │  ├───┼───┼───┼───┤               │\n",
    "  │  │ 2 │ 5 │ ? │ 5 │ ← User2 似てる!│\n",
    "  │  ├───┼───┼───┼───┤               │\n",
    "  │  │ ? │ 3 │ 4 │ 4 │               │\n",
    "  │  └───┴───┴───┴───┘               │\n",
    "  │     ↑                             │\n",
    "  │   個別の類似性を計算                │\n",
    "  └─────────────────────────────────────┘\n",
    "\n",
    "マクロ視点（行列因子分解）:\n",
    "  ┌─────────────────────────────────────┐\n",
    "  │  R ≈ P × Q                          │\n",
    "  │                                     │\n",
    "  │  全体を見て「潜在因子」を発見        │\n",
    "  │  例: ジャンル傾向、好みの傾向        │\n",
    "  │                                     │\n",
    "  │  P: 各ユーザーの潜在因子への重み    │\n",
    "  │  Q: 各アイテムの潜在因子への重み    │\n",
    "  └─────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3つの手法を実際に比較\n",
    "print(\"=== 3つの手法の比較 ===\")\n",
    "print()\n",
    "\n",
    "# 評価値行列\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],\n",
    "    [2, 5, 0, 5],\n",
    "    [0, 3, 4, 4],\n",
    "    [4, 2, 3, 0]\n",
    "])\n",
    "\n",
    "print(\"元の評価値行列 R:\")\n",
    "print(R)\n",
    "print()\n",
    "\n",
    "# 予測対象: r_{4,4}（ユーザー4のアイテム4への評価）\n",
    "target_user = 3  # 0-indexed\n",
    "target_item = 3  # 0-indexed\n",
    "\n",
    "print(f\"予測対象: r_{{4,4}}（ユーザー4のアイテム4への評価）\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "\n",
    "# ① ユーザーベース\n",
    "print(\"① ユーザーベース協調フィルタリング\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# ユーザー4と他のユーザーの類似度\n",
    "user4 = R[3]\n",
    "similarities_user = []\n",
    "for u in range(3):\n",
    "    sim = cosine_similarity_centered(R[u], user4)\n",
    "    similarities_user.append(sim)\n",
    "    print(f\"  sim(user4, user{u+1}) = {sim:.4f}\")\n",
    "\n",
    "# 予測値計算\n",
    "mask = R[:, target_item] != 0\n",
    "numerator = sum(similarities_user[u] * R[u, target_item] for u in range(3) if mask[u])\n",
    "denominator = sum(abs(similarities_user[u]) for u in range(3) if mask[u])\n",
    "pred_user = numerator / denominator if denominator != 0 else 0\n",
    "print(f\"  → 予測値: {pred_user:.4f}\")\n",
    "print()\n",
    "\n",
    "# ② アイテムベース\n",
    "print(\"② アイテムベース協調フィルタリング\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# アイテム4と他のアイテムの類似度\n",
    "similarities_item = []\n",
    "for i in range(3):\n",
    "    sim = cosine_similarity_item_based(R, i, target_item)\n",
    "    similarities_item.append(sim)\n",
    "    print(f\"  sim(item4, item{i+1}) = {sim:.4f}\")\n",
    "\n",
    "# 予測値計算\n",
    "mask = R[target_user, :] != 0\n",
    "numerator = sum(similarities_item[i] * R[target_user, i] for i in range(3) if mask[i])\n",
    "denominator = sum(abs(similarities_item[i]) for i in range(3) if mask[i])\n",
    "pred_item = numerator / denominator if denominator != 0 else 0\n",
    "print(f\"  → 予測値: {pred_item:.4f}\")\n",
    "print()\n",
    "\n",
    "# ③ 行列因子分解\n",
    "print(\"③ 行列因子分解\")\n",
    "print(\"-\"*40)\n",
    "mf = MatrixFactorization(n_factors=2, learning_rate=0.01, n_epochs=1000)\n",
    "mf.fit(R)\n",
    "R_hat = mf.predict()\n",
    "pred_mf = R_hat[target_user, target_item]\n",
    "print(f\"  最終損失: {mf.loss_history[-1]:.4f}\")\n",
    "print(f\"  → 予測値: {pred_mf:.4f}\")\n",
    "print()\n",
    "\n",
    "# 比較まとめ\n",
    "print(\"=\"*50)\n",
    "print(\"予測値の比較\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  ① ユーザーベース: {pred_user:.4f}\")\n",
    "print(f\"  ② アイテムベース: {pred_item:.4f}\")\n",
    "print(f\"  ③ 行列因子分解:   {pred_mf:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-20 本章で得られた学び\n",
    "\n",
    "## AIの予測のプロセス\n",
    "\n",
    "本章で学んだ「評価値予測」は、以下の4ステップで構成されます:\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                    AIの予測プロセス                            │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                │\n",
    "│  ① 問題を発見する                                             │\n",
    "│     ├── 何を予測したいか？                                    │\n",
    "│     └── 「ユーザーがアイテムにつける評価値を予測したい」       │\n",
    "│                    ↓                                          │\n",
    "│  ② 数理モデルを作る                                           │\n",
    "│     ├── 予測のための数式を定義                                │\n",
    "│     ├── 協調フィルタリング: 類似度ベースの予測式              │\n",
    "│     └── 行列因子分解: R ≈ P × Q                               │\n",
    "│                    ↓                                          │\n",
    "│  ③ データからパラメータを推定する                             │\n",
    "│     ├── 損失関数を定義                                        │\n",
    "│     ├── 勾配降下法でパラメータを最適化                        │\n",
    "│     └── P, Q の値を学習                                       │\n",
    "│                    ↓                                          │\n",
    "│  ④ 予測値を出力する                                           │\n",
    "│     └── 学習したモデルで欠損値を予測                          │\n",
    "│                                                                │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 本章で学んだ数学\n",
    "\n",
    "| 概念 | 記号・式 | 役割 |\n",
    "|------|----------|------|\n",
    "| 内積 | $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_i a_i b_i$ | ベクトル間の関係を数値化 |\n",
    "| ノルム | $\\|\\mathbf{a}\\| = \\sqrt{\\sum_i a_i^2}$ | ベクトルの大きさ |\n",
    "| コサイン類似度 | $\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$ | 類似性の尺度 |\n",
    "| 行列積 | $(AB)_{ij} = \\sum_k A_{ik} B_{kj}$ | 行列因子分解の基礎 |\n",
    "| 損失関数 | $L = \\sum_{(u,i)} (r_{u,i} - \\hat{r}_{u,i})^2$ | 予測の良さの指標 |\n",
    "| 偏微分 | $\\frac{\\partial L}{\\partial p_{u,k}}$ | パラメータごとの勾配 |\n",
    "| 勾配降下法 | $p_{u,k} \\leftarrow p_{u,k} - \\eta \\frac{\\partial L}{\\partial p_{u,k}}$ | パラメータの更新 |\n",
    "\n",
    "## 本章で実装した関数\n",
    "\n",
    "```python\n",
    "# 類似度計算\n",
    "cosine_similarity(a, b)                    # 基本的なコサイン類似度\n",
    "cosine_similarity_centered(a, b)           # 中心化コサイン類似度\n",
    "cosine_similarity_item_based(R, i, j)      # アイテムベースの類似度\n",
    "\n",
    "# 損失関数\n",
    "compute_loss(R, P, Q)                      # 二乗誤差損失\n",
    "\n",
    "# 行列因子分解\n",
    "class MatrixFactorization:\n",
    "    fit(R)          # 学習\n",
    "    predict()       # 予測\n",
    "    plot_loss()     # 損失推移の可視化\n",
    "```\n",
    "\n",
    "## 次のステップ\n",
    "\n",
    "この数学的基礎を理解した上で、以下の発展的なトピックに進むことができます:\n",
    "\n",
    "1. **正則化**: 過学習を防ぐための L2 正則化項の追加\n",
    "2. **暗黙的フィードバック**: クリックや閲覧履歴からの推薦\n",
    "3. **ディープラーニング**: ニューラルネットワークを用いた推薦システム\n",
    "4. **実装への応用**: MCPサーバーへの組み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 本章の学習成果を確認\n",
    "print(\"=\"*60)\n",
    "print(\"       第2章 完了おめでとうございます！\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"本Notebookで学んだこと:\")\n",
    "print()\n",
    "print(\"  ✓ 評価値行列 R の構造と意味\")\n",
    "print(\"  ✓ コサイン類似度による類似性計算\")\n",
    "print(\"  ✓ 中心化による評価バイアスの除去\")\n",
    "print(\"  ✓ ユーザーベース協調フィルタリング\")\n",
    "print(\"  ✓ アイテムベース協調フィルタリング\")\n",
    "print(\"  ✓ 行列因子分解 R ≈ P × Q\")\n",
    "print(\"  ✓ 損失関数と最小二乗法\")\n",
    "print(\"  ✓ 微分・偏微分の基礎\")\n",
    "print(\"  ✓ 勾配降下法によるパラメータ最適化\")\n",
    "print()\n",
    "print(\"これらの知識を使って、実際の推薦システムを\")\n",
    "print(\"構築する準備ができました！\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 補足: なぜ行列因子分解が強力なのか？\n",
    "\n",
    "## 協調フィルタリングの限界\n",
    "\n",
    "協調フィルタリングには以下の問題があります:\n",
    "\n",
    "### 1. スパース性問題（Sparsity Problem）\n",
    "現実の評価値行列は非常にスパース（疎）です。Amazonでは数百万アイテムがありますが、\n",
    "ユーザーが評価するのはその0.01%未満です。\n",
    "\n",
    "```\n",
    "理想:                        現実:\n",
    "[5, 3, 4, 2, 5]              [5, 0, 0, 0, 0]\n",
    "[4, 5, 3, 5, 2]              [0, 0, 0, 5, 0]\n",
    "[3, 2, 5, 4, 3]              [0, 3, 0, 0, 0]\n",
    "                             → 類似度計算が困難!\n",
    "```\n",
    "\n",
    "### 2. コールドスタート問題（Cold Start Problem）\n",
    "- 新規ユーザー: 評価履歴がないため類似ユーザーを見つけられない\n",
    "- 新規アイテム: 誰にも評価されていないため推薦できない\n",
    "\n",
    "## 行列因子分解の強み\n",
    "\n",
    "行列因子分解はこれらの問題を**潜在因子（Latent Factors）**で緩和します:\n",
    "\n",
    "```\n",
    "P (ユーザー因子):            Q (アイテム因子):\n",
    "User1: [アクション好き, SF好き]   Item1: [アクション強, SF弱]\n",
    "User2: [アクション弱, SF強]      Item2: [アクション弱, SF強]\n",
    "\n",
    "→ User2とItem2は直接の評価がなくても、\n",
    "  「SF好き」という共通因子で高い予測値が出る！\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# スパース行列での比較実験\n",
    "print(\"=== スパース行列での手法比較 ===\")\n",
    "print()\n",
    "\n",
    "# 非常にスパースな行列（実際の推薦システムに近い）\n",
    "R_sparse = np.array([\n",
    "    [5, 0, 0, 0, 0, 0, 4, 0],\n",
    "    [0, 0, 3, 0, 0, 5, 0, 0],\n",
    "    [0, 4, 0, 0, 2, 0, 0, 0],\n",
    "    [0, 0, 0, 5, 0, 0, 0, 3],\n",
    "    [2, 0, 0, 0, 0, 0, 0, 4],\n",
    "    [0, 0, 4, 0, 0, 3, 0, 0]\n",
    "])\n",
    "\n",
    "print(\"スパースな評価値行列 R:\")\n",
    "print(R_sparse)\n",
    "print()\n",
    "\n",
    "# スパース率を計算\n",
    "total = R_sparse.size\n",
    "non_zero = np.count_nonzero(R_sparse)\n",
    "sparsity = 1 - (non_zero / total)\n",
    "print(f\"スパース率: {sparsity:.1%} ({total - non_zero}/{total} が欠損)\")\n",
    "print()\n",
    "\n",
    "# 行列因子分解で全ての欠損値を予測\n",
    "print(\"行列因子分解による予測:\")\n",
    "mf_sparse = MatrixFactorization(n_factors=3, learning_rate=0.01, n_epochs=2000)\n",
    "mf_sparse.fit(R_sparse)\n",
    "R_hat_sparse = mf_sparse.predict()\n",
    "\n",
    "print(f\"最終損失: {mf_sparse.loss_history[-1]:.4f}\")\n",
    "print()\n",
    "print(\"予測された完全な行列 R̂:\")\n",
    "print(np.round(R_hat_sparse, 1))\n",
    "print()\n",
    "\n",
    "# 元の評価値との比較\n",
    "print(\"元の評価値（0以外）との比較:\")\n",
    "mask = R_sparse != 0\n",
    "mae = np.mean(np.abs(R_sparse[mask] - R_hat_sparse[mask]))\n",
    "print(f\"平均絶対誤差 (MAE): {mae:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 補足: 正則化（Regularization）\n",
    "\n",
    "## 過学習の問題\n",
    "\n",
    "行列因子分解で学習を続けると、訓練データに対する損失は0に近づきます。\n",
    "しかし、これは**過学習（Overfitting）**を引き起こす可能性があります。\n",
    "\n",
    "```\n",
    "過学習:\n",
    "- 訓練データ: 完璧に予測 ✓\n",
    "- 未知のデータ: 予測がズレる ✗\n",
    "```\n",
    "\n",
    "## L2正則化\n",
    "\n",
    "正則化項を損失関数に追加して、パラメータが大きくなりすぎるのを防ぎます:\n",
    "\n",
    "$$L_{\\text{reg}} = \\sum_{(u,i) \\in \\mathcal{O}} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda \\left( \\sum_{u,k} p_{u,k}^2 + \\sum_{k,i} q_{k,i}^2 \\right)$$\n",
    "\n",
    "- $\\lambda$: 正則化パラメータ（ハイパーパラメータ）\n",
    "- 大きな$\\lambda$: 正則化が強い → 過学習を防ぐが、学習が不十分になる可能性\n",
    "- 小さな$\\lambda$: 正則化が弱い → 過学習のリスク\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 正則化付き行列因子分解\n",
    "class MatrixFactorizationRegularized:\n",
    "    \"\"\"\n",
    "    L2正則化付き行列因子分解\n",
    "    \n",
    "    損失関数:\n",
    "    L = Σ (r - r̂)² + λ(Σp² + Σq²)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_factors=2, learning_rate=0.01, n_epochs=100, reg_lambda=0.01):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.reg_lambda = reg_lambda  # 正則化パラメータ\n",
    "        self.P = None\n",
    "        self.Q = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def fit(self, R):\n",
    "        n_users, n_items = R.shape\n",
    "        \n",
    "        # 初期化\n",
    "        np.random.seed(42)\n",
    "        self.P = np.random.rand(n_users, self.n_factors) * 0.1\n",
    "        self.Q = np.random.rand(self.n_factors, n_items) * 0.1\n",
    "        \n",
    "        mask = (R != 0)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            R_hat = self.P @ self.Q\n",
    "            E = (R - R_hat) * mask\n",
    "            \n",
    "            # 正則化項を含む損失\n",
    "            loss = np.sum(E**2) + self.reg_lambda * (np.sum(self.P**2) + np.sum(self.Q**2))\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # 勾配計算（正則化項の微分を含む）\n",
    "            grad_P = -2 * E @ self.Q.T + 2 * self.reg_lambda * self.P\n",
    "            grad_Q = -2 * self.P.T @ E + 2 * self.reg_lambda * self.Q\n",
    "            \n",
    "            # パラメータ更新\n",
    "            self.P -= self.learning_rate * grad_P\n",
    "            self.Q -= self.learning_rate * grad_Q\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self):\n",
    "        return self.P @ self.Q\n",
    "\n",
    "print(\"MatrixFactorizationRegularized クラスを定義しました\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 正則化の効果を比較\n",
    "print(\"=== 正則化の効果 ===\")\n",
    "print()\n",
    "\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],\n",
    "    [2, 5, 0, 5],\n",
    "    [0, 3, 4, 4],\n",
    "    [4, 2, 3, 0]\n",
    "])\n",
    "\n",
    "# 正則化なし vs あり\n",
    "mf_no_reg = MatrixFactorization(n_factors=2, learning_rate=0.01, n_epochs=1000)\n",
    "mf_with_reg = MatrixFactorizationRegularized(n_factors=2, learning_rate=0.01, n_epochs=1000, reg_lambda=0.1)\n",
    "\n",
    "mf_no_reg.fit(R)\n",
    "mf_with_reg.fit(R)\n",
    "\n",
    "print(\"正則化なし:\")\n",
    "print(f\"  最終損失: {mf_no_reg.loss_history[-1]:.4f}\")\n",
    "print(f\"  Pのノルム: {np.linalg.norm(mf_no_reg.P):.4f}\")\n",
    "print(f\"  Qのノルム: {np.linalg.norm(mf_no_reg.Q):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"正則化あり (λ=0.1):\")\n",
    "print(f\"  最終損失: {mf_with_reg.loss_history[-1]:.4f}\")\n",
    "print(f\"  Pのノルム: {np.linalg.norm(mf_with_reg.P):.4f}\")\n",
    "print(f\"  Qのノルム: {np.linalg.norm(mf_with_reg.Q):.4f}\")\n",
    "print()\n",
    "\n",
    "# 予測値の比較\n",
    "print(\"予測行列の比較:\")\n",
    "print(\"\\n正則化なし:\")\n",
    "print(np.round(mf_no_reg.predict(), 2))\n",
    "print(\"\\n正則化あり:\")\n",
    "print(np.round(mf_with_reg.predict(), 2))\n",
    "print(\"\\n→ 正則化ありの方がパラメータのノルムが小さく、\")\n",
    "print(\"  汎化性能が高い（未知のデータに対して良い予測をする）可能性があります。\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 補足: ハイパーパラメータチューニング\n",
    "\n",
    "行列因子分解には複数のハイパーパラメータがあります:\n",
    "\n",
    "| パラメータ | 意味 | 典型的な範囲 |\n",
    "|-----------|------|-------------|\n",
    "| d (n_factors) | 潜在因子の数 | 10〜200 |\n",
    "| η (learning_rate) | 学習率 | 0.001〜0.1 |\n",
    "| epochs | 学習回数 | 100〜10000 |\n",
    "| λ (reg_lambda) | 正則化パラメータ | 0.001〜0.1 |\n",
    "\n",
    "これらは**交差検証（Cross Validation）**で最適値を探します。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 学習率の比較\n",
    "print(\"=== 学習率の影響 ===\")\n",
    "print()\n",
    "\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],\n",
    "    [2, 5, 0, 5],\n",
    "    [0, 3, 4, 4],\n",
    "    [4, 2, 3, 0]\n",
    "])\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    mf = MatrixFactorization(n_factors=2, learning_rate=lr, n_epochs=500)\n",
    "    mf.fit(R)\n",
    "    plt.plot(mf.loss_history, color=color, label=f'η={lr}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('エポック数')\n",
    "plt.ylabel('損失 L')\n",
    "plt.title('学習率による収束速度の違い')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # 対数スケール\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"考察:\")\n",
    "print(\"  - 学習率が小さい (η=0.001): 収束が遅い\")\n",
    "print(\"  - 学習率が適切 (η=0.01〜0.05): スムーズに収束\")\n",
    "print(\"  - 学習率が大きすぎる (η=0.1): 不安定になる可能性\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 補足: 確率的勾配降下法（SGD）\n",
    "\n",
    "本章で実装した勾配降下法は**バッチ勾配降下法**と呼ばれます。\n",
    "これは全てのデータを使って勾配を計算します。\n",
    "\n",
    "実際の大規模システムでは、**確率的勾配降下法（SGD）**がよく使われます:\n",
    "\n",
    "```python\n",
    "# バッチ勾配降下法（本章の実装）\n",
    "for epoch in range(n_epochs):\n",
    "    # 全データで勾配計算\n",
    "    grad = compute_gradient(all_data)\n",
    "    update_parameters(grad)\n",
    "\n",
    "# 確率的勾配降下法（SGD）\n",
    "for epoch in range(n_epochs):\n",
    "    for (user, item, rating) in shuffle(data):\n",
    "        # 1サンプルで勾配計算\n",
    "        grad = compute_gradient_single(user, item, rating)\n",
    "        update_parameters(grad)\n",
    "```\n",
    "\n",
    "### SGDのメリット\n",
    "1. **メモリ効率**: 全データをメモリに載せる必要がない\n",
    "2. **計算速度**: 1エポックあたりの計算は早い\n",
    "3. **オンライン学習**: 新しいデータが来たらすぐに学習できる\n",
    "\n",
    "### ミニバッチSGD\n",
    "バッチとSGDの中間。32〜256サンプルずつ処理:\n",
    "- バッチのノイズ軽減効果\n",
    "- SGDの計算効率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 実践的なTips\n",
    "\n",
    "## 1. 評価指標\n",
    "\n",
    "推薦システムの性能を測る指標:\n",
    "\n",
    "| 指標 | 説明 | 式 |\n",
    "|------|------|----|\n",
    "| RMSE | 二乗平均平方根誤差 | $\\sqrt{\\frac{1}{n}\\sum(r - \\hat{r})^2}$ |\n",
    "| MAE | 平均絶対誤差 | $\\frac{1}{n}\\sum|r - \\hat{r}|$ |\n",
    "| Precision@k | 上位k件の適合率 | 関連アイテム数 / k |\n",
    "| Recall@k | 上位k件の再現率 | 上位k件の関連アイテム数 / 全関連アイテム数 |\n",
    "\n",
    "## 2. データの前処理\n",
    "\n",
    "```python\n",
    "# 評価値のスケーリング（1-5 → 0-1）\n",
    "R_scaled = (R - 1) / 4\n",
    "\n",
    "# グローバルバイアスの除去\n",
    "global_mean = np.mean(R[R != 0])\n",
    "R_centered = R - global_mean  # 0は変えない\n",
    "```\n",
    "\n",
    "## 3. 実装時の注意点\n",
    "\n",
    "- **数値安定性**: 勾配爆発を防ぐため、勾配クリッピングを使う\n",
    "- **早期停止**: 検証データの損失が上がり始めたら学習を止める\n",
    "- **バイアス項**: ユーザーバイアス・アイテムバイアスを追加すると精度向上\n",
    "\n",
    "```python\n",
    "# バイアス付き予測\n",
    "r̂_ui = μ + b_u + b_i + p_u · q_i\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 評価指標の実装\n",
    "def rmse(R_true, R_pred, mask):\n",
    "    \"\"\"Root Mean Squared Error\"\"\"\n",
    "    error = R_true - R_pred\n",
    "    return np.sqrt(np.mean(error[mask]**2))\n",
    "\n",
    "def mae(R_true, R_pred, mask):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    error = R_true - R_pred\n",
    "    return np.mean(np.abs(error[mask]))\n",
    "\n",
    "# 使用例\n",
    "R = np.array([\n",
    "    [2, 3, 0, 5],\n",
    "    [2, 5, 0, 5],\n",
    "    [0, 3, 4, 4],\n",
    "    [4, 2, 3, 0]\n",
    "])\n",
    "\n",
    "mf = MatrixFactorization(n_factors=2, learning_rate=0.01, n_epochs=1000)\n",
    "mf.fit(R)\n",
    "R_hat = mf.predict()\n",
    "\n",
    "mask = R != 0\n",
    "\n",
    "print(\"=== 評価指標 ===\")\n",
    "print(f\"RMSE: {rmse(R, R_hat, mask):.4f}\")\n",
    "print(f\"MAE:  {mae(R, R_hat, mask):.4f}\")\n",
    "print()\n",
    "print(\"目安:\")\n",
    "print(\"  RMSE < 1.0: 良好な予測\")\n",
    "print(\"  RMSE < 0.5: 優れた予測\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}